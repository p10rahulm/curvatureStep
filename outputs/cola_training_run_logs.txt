# ----------------------------------------------------------------------------------------------------
# Running 10 epochs of training - 10 runs
# ----------------------------------------------------------------------------------------------------

Running CoLA Training with Optimizer = SimpleSGD
params= {'lr': 0.001}
Running Loop: 1/10
Epoch 1, Loss: 0.6542
Epoch 2, Loss: 0.6300
Epoch 3, Loss: 0.6226
Epoch 4, Loss: 0.6189
Epoch 5, Loss: 0.6169
Epoch 6, Loss: 0.6156
Epoch 7, Loss: 0.6148
Epoch 8, Loss: 0.6140
Epoch 9, Loss: 0.6134
Epoch 10, Loss: 0.6128
Test set: Average loss: 0.0109, Accuracy: 353/515 (68.54%)
accuracy= 0.6854368932038835
accuracies= [0.6854368932038835]
Running Loop: 2/10
Epoch 1, Loss: 0.6687
Epoch 2, Loss: 0.6328
Epoch 3, Loss: 0.6225
Epoch 4, Loss: 0.6181
Epoch 5, Loss: 0.6161
Epoch 6, Loss: 0.6148
Epoch 7, Loss: 0.6138
Epoch 8, Loss: 0.6131
Epoch 9, Loss: 0.6125
Epoch 10, Loss: 0.6123
Test set: Average loss: 0.0109, Accuracy: 353/515 (68.54%)
accuracy= 0.6854368932038835
accuracies= [0.6854368932038835, 0.6854368932038835]
Running Loop: 3/10
Epoch 1, Loss: 0.6583
Epoch 2, Loss: 0.6299
Epoch 3, Loss: 0.6222
Epoch 4, Loss: 0.6190
Epoch 5, Loss: 0.6168
Epoch 6, Loss: 0.6160
Epoch 7, Loss: 0.6149
Epoch 8, Loss: 0.6139
Epoch 9, Loss: 0.6136
Epoch 10, Loss: 0.6127
Test set: Average loss: 0.0109, Accuracy: 354/515 (68.74%)
accuracy= 0.6873786407766991
accuracies= [0.6854368932038835, 0.6854368932038835, 0.6873786407766991]
Running Loop: 4/10
Epoch 1, Loss: 0.6646
Epoch 2, Loss: 0.6277
Epoch 3, Loss: 0.6190
Epoch 4, Loss: 0.6154
Epoch 5, Loss: 0.6139
Epoch 6, Loss: 0.6127
Epoch 7, Loss: 0.6120
Epoch 8, Loss: 0.6113
Epoch 9, Loss: 0.6107
Epoch 10, Loss: 0.6106
Test set: Average loss: 0.0109, Accuracy: 353/515 (68.54%)
accuracy= 0.6854368932038835
accuracies= [0.6854368932038835, 0.6854368932038835, 0.6873786407766991, 0.6854368932038835]
Running Loop: 5/10
Epoch 1, Loss: 0.6710
Epoch 2, Loss: 0.6330
Epoch 3, Loss: 0.6228
Epoch 4, Loss: 0.6187
Epoch 5, Loss: 0.6164
Epoch 6, Loss: 0.6149
Epoch 7, Loss: 0.6140
Epoch 8, Loss: 0.6138
Epoch 9, Loss: 0.6128
Epoch 10, Loss: 0.6122
Test set: Average loss: 0.0109, Accuracy: 353/515 (68.54%)
accuracy= 0.6854368932038835
accuracies= [0.6854368932038835, 0.6854368932038835, 0.6873786407766991, 0.6854368932038835, 0.6854368932038835]
Running Loop: 6/10
Epoch 1, Loss: 0.6575
Epoch 2, Loss: 0.6295
Epoch 3, Loss: 0.6212
Epoch 4, Loss: 0.6178
Epoch 5, Loss: 0.6159
Epoch 6, Loss: 0.6148
Epoch 7, Loss: 0.6138
Epoch 8, Loss: 0.6131
Epoch 9, Loss: 0.6126
Epoch 10, Loss: 0.6126
Test set: Average loss: 0.0109, Accuracy: 354/515 (68.74%)
accuracy= 0.6873786407766991
accuracies= [0.6854368932038835, 0.6854368932038835, 0.6873786407766991, 0.6854368932038835, 0.6854368932038835, 0.6873786407766991]
Running Loop: 7/10
Epoch 1, Loss: 0.6739
Epoch 2, Loss: 0.6346
Epoch 3, Loss: 0.6231
Epoch 4, Loss: 0.6184
Epoch 5, Loss: 0.6160
Epoch 6, Loss: 0.6147
Epoch 7, Loss: 0.6134
Epoch 8, Loss: 0.6131
Epoch 9, Loss: 0.6123
Epoch 10, Loss: 0.6119
Test set: Average loss: 0.0109, Accuracy: 353/515 (68.54%)
accuracy= 0.6854368932038835
accuracies= [0.6854368932038835, 0.6854368932038835, 0.6873786407766991, 0.6854368932038835, 0.6854368932038835, 0.6873786407766991, 0.6854368932038835]
Running Loop: 8/10
Epoch 1, Loss: 0.6492
Epoch 2, Loss: 0.6255
Epoch 3, Loss: 0.6190
Epoch 4, Loss: 0.6164
Epoch 5, Loss: 0.6147
Epoch 6, Loss: 0.6137
Epoch 7, Loss: 0.6132
Epoch 8, Loss: 0.6122
Epoch 9, Loss: 0.6117
Epoch 10, Loss: 0.6118
Test set: Average loss: 0.0109, Accuracy: 353/515 (68.54%)
accuracy= 0.6854368932038835
accuracies= [0.6854368932038835, 0.6854368932038835, 0.6873786407766991, 0.6854368932038835, 0.6854368932038835, 0.6873786407766991, 0.6854368932038835, 0.6854368932038835]
Running Loop: 9/10
Epoch 1, Loss: 0.6531
Epoch 2, Loss: 0.6305
Epoch 3, Loss: 0.6227
Epoch 4, Loss: 0.6188
Epoch 5, Loss: 0.6166
Epoch 6, Loss: 0.6155
Epoch 7, Loss: 0.6142
Epoch 8, Loss: 0.6135
Epoch 9, Loss: 0.6130
Epoch 10, Loss: 0.6127
Test set: Average loss: 0.0109, Accuracy: 351/515 (68.16%)
accuracy= 0.6815533980582524
accuracies= [0.6854368932038835, 0.6854368932038835, 0.6873786407766991, 0.6854368932038835, 0.6854368932038835, 0.6873786407766991, 0.6854368932038835, 0.6854368932038835, 0.6815533980582524]
Running Loop: 10/10
Epoch 1, Loss: 0.6686
Epoch 2, Loss: 0.6311
Epoch 3, Loss: 0.6215
Epoch 4, Loss: 0.6177
Epoch 5, Loss: 0.6159
Epoch 6, Loss: 0.6146
Epoch 7, Loss: 0.6134
Epoch 8, Loss: 0.6128
Epoch 9, Loss: 0.6127
Epoch 10, Loss: 0.6122
Test set: Average loss: 0.0109, Accuracy: 352/515 (68.35%)
accuracy= 0.683495145631068
accuracies= [0.6854368932038835, 0.6854368932038835, 0.6873786407766991, 0.6854368932038835, 0.6854368932038835, 0.6873786407766991, 0.6854368932038835, 0.6854368932038835, 0.6815533980582524, 0.683495145631068]

Running CoLA Training with Optimizer = SimpleSGDCurvature
params= {'lr': 0.001, 'epsilon': 0.01}
Running Loop: 1/10
Epoch 1, Loss: 0.6119
Epoch 2, Loss: 0.6083
Epoch 3, Loss: 0.6072
Epoch 4, Loss: 0.6056
Epoch 5, Loss: 0.6040
Epoch 6, Loss: 0.6024
Epoch 7, Loss: 0.6002
Epoch 8, Loss: 0.5974
Epoch 9, Loss: 0.5941
Epoch 10, Loss: 0.5906
Test set: Average loss: 0.0107, Accuracy: 345/515 (66.99%)
accuracy= 0.6699029126213593
accuracies= [0.6699029126213593]
Running Loop: 2/10
Epoch 1, Loss: 0.6115
Epoch 2, Loss: 0.6075
Epoch 3, Loss: 0.6058
Epoch 4, Loss: 0.6039
Epoch 5, Loss: 0.6022
Epoch 6, Loss: 0.5997
Epoch 7, Loss: 0.5971
Epoch 8, Loss: 0.5944
Epoch 9, Loss: 0.5910
Epoch 10, Loss: 0.5892
Test set: Average loss: 0.0111, Accuracy: 344/515 (66.80%)
accuracy= 0.6679611650485436
accuracies= [0.6699029126213593, 0.6679611650485436]
Running Loop: 3/10
Epoch 1, Loss: 0.6116
Epoch 2, Loss: 0.6082
Epoch 3, Loss: 0.6069
Epoch 4, Loss: 0.6059
Epoch 5, Loss: 0.6039
Epoch 6, Loss: 0.6022
Epoch 7, Loss: 0.5995
Epoch 8, Loss: 0.5955
Epoch 9, Loss: 0.5923
Epoch 10, Loss: 0.5884
Test set: Average loss: 0.0113, Accuracy: 339/515 (65.83%)
accuracy= 0.658252427184466
accuracies= [0.6699029126213593, 0.6679611650485436, 0.658252427184466]
Running Loop: 4/10
Epoch 1, Loss: 0.6110
Epoch 2, Loss: 0.6074
Epoch 3, Loss: 0.6065
Epoch 4, Loss: 0.6049
Epoch 5, Loss: 0.6038
Epoch 6, Loss: 0.6013
Epoch 7, Loss: 0.5995
Epoch 8, Loss: 0.5970
Epoch 9, Loss: 0.5936
Epoch 10, Loss: 0.5909
Test set: Average loss: 0.0107, Accuracy: 349/515 (67.77%)
accuracy= 0.6776699029126214
accuracies= [0.6699029126213593, 0.6679611650485436, 0.658252427184466, 0.6776699029126214]
Running Loop: 5/10
Epoch 1, Loss: 0.6115
Epoch 2, Loss: 0.6078
Epoch 3, Loss: 0.6060
Epoch 4, Loss: 0.6046
Epoch 5, Loss: 0.6020
Epoch 6, Loss: 0.5996
Epoch 7, Loss: 0.5969
Epoch 8, Loss: 0.5944
Epoch 9, Loss: 0.5910
Epoch 10, Loss: 0.5874
Test set: Average loss: 0.0106, Accuracy: 342/515 (66.41%)
accuracy= 0.6640776699029126
accuracies= [0.6699029126213593, 0.6679611650485436, 0.658252427184466, 0.6776699029126214, 0.6640776699029126]
Running Loop: 6/10
Epoch 1, Loss: 0.6110
Epoch 2, Loss: 0.6070
Epoch 3, Loss: 0.6052
Epoch 4, Loss: 0.6031
Epoch 5, Loss: 0.6003
Epoch 6, Loss: 0.5981
Epoch 7, Loss: 0.5945
Epoch 8, Loss: 0.5918
Epoch 9, Loss: 0.5887
Epoch 10, Loss: 0.5864
Test set: Average loss: 0.0114, Accuracy: 331/515 (64.27%)
accuracy= 0.6427184466019418
accuracies= [0.6699029126213593, 0.6679611650485436, 0.658252427184466, 0.6776699029126214, 0.6640776699029126, 0.6427184466019418]
Running Loop: 7/10
Epoch 1, Loss: 0.6107
Epoch 2, Loss: 0.6078
Epoch 3, Loss: 0.6064
Epoch 4, Loss: 0.6049
Epoch 5, Loss: 0.6030
Epoch 6, Loss: 0.6005
Epoch 7, Loss: 0.5981
Epoch 8, Loss: 0.5958
Epoch 9, Loss: 0.5926
Epoch 10, Loss: 0.5898
Test set: Average loss: 0.0106, Accuracy: 350/515 (67.96%)
accuracy= 0.6796116504854369
accuracies= [0.6699029126213593, 0.6679611650485436, 0.658252427184466, 0.6776699029126214, 0.6640776699029126, 0.6427184466019418, 0.6796116504854369]
Running Loop: 8/10
Epoch 1, Loss: 0.6104
Epoch 2, Loss: 0.6076
Epoch 3, Loss: 0.6064
Epoch 4, Loss: 0.6056
Epoch 5, Loss: 0.6039
Epoch 6, Loss: 0.6024
Epoch 7, Loss: 0.6000
Epoch 8, Loss: 0.5973
Epoch 9, Loss: 0.5947
Epoch 10, Loss: 0.5923
Test set: Average loss: 0.0117, Accuracy: 322/515 (62.52%)
accuracy= 0.625242718446602
accuracies= [0.6699029126213593, 0.6679611650485436, 0.658252427184466, 0.6776699029126214, 0.6640776699029126, 0.6427184466019418, 0.6796116504854369, 0.625242718446602]
Running Loop: 9/10
Epoch 1, Loss: 0.6114
Epoch 2, Loss: 0.6081
Epoch 3, Loss: 0.6068
Epoch 4, Loss: 0.6048
Epoch 5, Loss: 0.6025
Epoch 6, Loss: 0.6006
Epoch 7, Loss: 0.5978
Epoch 8, Loss: 0.5951
Epoch 9, Loss: 0.5930
Epoch 10, Loss: 0.5902
Test set: Average loss: 0.0111, Accuracy: 345/515 (66.99%)
accuracy= 0.6699029126213593
accuracies= [0.6699029126213593, 0.6679611650485436, 0.658252427184466, 0.6776699029126214, 0.6640776699029126, 0.6427184466019418, 0.6796116504854369, 0.625242718446602, 0.6699029126213593]
Running Loop: 10/10
Epoch 1, Loss: 0.6115
Epoch 2, Loss: 0.6080
Epoch 3, Loss: 0.6062
Epoch 4, Loss: 0.6048
Epoch 5, Loss: 0.6030
Epoch 6, Loss: 0.6011
Epoch 7, Loss: 0.5978
Epoch 8, Loss: 0.5946
Epoch 9, Loss: 0.5927
Epoch 10, Loss: 0.5895
Test set: Average loss: 0.0110, Accuracy: 349/515 (67.77%)
accuracy= 0.6776699029126214
accuracies= [0.6699029126213593, 0.6679611650485436, 0.658252427184466, 0.6776699029126214, 0.6640776699029126, 0.6427184466019418, 0.6796116504854369, 0.625242718446602, 0.6699029126213593, 0.6776699029126214]

Running CoLA Training with Optimizer = Adam
params= {'lr': 0.001, 'betas': (0.9, 0.999)}
Running Loop: 1/10
Epoch 1, Loss: 0.6107
Epoch 2, Loss: 0.6083
Epoch 3, Loss: 0.6066
Epoch 4, Loss: 0.6068
Epoch 5, Loss: 0.5983
Epoch 6, Loss: 0.5873
Epoch 7, Loss: 0.5794
Epoch 8, Loss: 0.5609
Epoch 9, Loss: 0.5462
Epoch 10, Loss: 0.5290
Test set: Average loss: 0.0114, Accuracy: 334/515 (64.85%)
accuracy= 0.6485436893203883
accuracies= [0.6485436893203883]
Running Loop: 2/10
Epoch 1, Loss: 0.6112
Epoch 2, Loss: 0.6076
Epoch 3, Loss: 0.6043
Epoch 4, Loss: 0.5991
Epoch 5, Loss: 0.5918
Epoch 6, Loss: 0.5790
Epoch 7, Loss: 0.5625
Epoch 8, Loss: 0.5489
Epoch 9, Loss: 0.5325
Epoch 10, Loss: 0.5205
Test set: Average loss: 0.0110, Accuracy: 344/515 (66.80%)
accuracy= 0.6679611650485436
accuracies= [0.6485436893203883, 0.6679611650485436]
Running Loop: 3/10
Epoch 1, Loss: 0.6105
Epoch 2, Loss: 0.6085
Epoch 3, Loss: 0.6063
Epoch 4, Loss: 0.6035
Epoch 5, Loss: 0.5978
Epoch 6, Loss: 0.5934
Epoch 7, Loss: 0.5803
Epoch 8, Loss: 0.5703
Epoch 9, Loss: 0.5513
Epoch 10, Loss: 0.5433
Test set: Average loss: 0.0115, Accuracy: 333/515 (64.66%)
accuracy= 0.6466019417475728
accuracies= [0.6485436893203883, 0.6679611650485436, 0.6466019417475728]
Running Loop: 4/10
Epoch 1, Loss: 0.6109
Epoch 2, Loss: 0.6082
Epoch 3, Loss: 0.6069
Epoch 4, Loss: 0.6053
Epoch 5, Loss: 0.6040
Epoch 6, Loss: 0.6043
Epoch 7, Loss: 0.5993
Epoch 8, Loss: 0.5907
Epoch 9, Loss: 0.5822
Epoch 10, Loss: 0.5687
Test set: Average loss: 0.0113, Accuracy: 335/515 (65.05%)
accuracy= 0.6504854368932039
accuracies= [0.6485436893203883, 0.6679611650485436, 0.6466019417475728, 0.6504854368932039]
Running Loop: 5/10
Epoch 1, Loss: 0.6102
Epoch 2, Loss: 0.6074
Epoch 3, Loss: 0.6063
Epoch 4, Loss: 0.6033
Epoch 5, Loss: 0.5968
Epoch 6, Loss: 0.5867
Epoch 7, Loss: 0.5678
Epoch 8, Loss: 0.5557
Epoch 9, Loss: 0.5342
Epoch 10, Loss: 0.5173
Test set: Average loss: 0.0122, Accuracy: 317/515 (61.55%)
accuracy= 0.6155339805825243
accuracies= [0.6485436893203883, 0.6679611650485436, 0.6466019417475728, 0.6504854368932039, 0.6155339805825243]
Running Loop: 6/10
Epoch 1, Loss: 0.6103
Epoch 2, Loss: 0.6062
Epoch 3, Loss: 0.6004
Epoch 4, Loss: 0.5939
Epoch 5, Loss: 0.5857
Epoch 6, Loss: 0.5769
Epoch 7, Loss: 0.5613
Epoch 8, Loss: 0.5450
Epoch 9, Loss: 0.5287
Epoch 10, Loss: 0.5153
Test set: Average loss: 0.0107, Accuracy: 339/515 (65.83%)
accuracy= 0.658252427184466
accuracies= [0.6485436893203883, 0.6679611650485436, 0.6466019417475728, 0.6504854368932039, 0.6155339805825243, 0.658252427184466]
Running Loop: 7/10
Epoch 1, Loss: 0.6102
Epoch 2, Loss: 0.6067
Epoch 3, Loss: 0.6052
Epoch 4, Loss: 0.6018
Epoch 5, Loss: 0.5911
Epoch 6, Loss: 0.5834
Epoch 7, Loss: 0.5655
Epoch 8, Loss: 0.5488
Epoch 9, Loss: 0.5308
Epoch 10, Loss: 0.5128
Test set: Average loss: 0.0105, Accuracy: 342/515 (66.41%)
accuracy= 0.6640776699029126
accuracies= [0.6485436893203883, 0.6679611650485436, 0.6466019417475728, 0.6504854368932039, 0.6155339805825243, 0.658252427184466, 0.6640776699029126]
Running Loop: 8/10
Epoch 1, Loss: 0.6103
Epoch 2, Loss: 0.6085
Epoch 3, Loss: 0.6078
Epoch 4, Loss: 0.6081
Epoch 5, Loss: 0.6039
Epoch 6, Loss: 0.6024
Epoch 7, Loss: 0.5939
Epoch 8, Loss: 0.5789
Epoch 9, Loss: 0.5612
Epoch 10, Loss: 0.5450
Test set: Average loss: 0.0114, Accuracy: 341/515 (66.21%)
accuracy= 0.6621359223300971
accuracies= [0.6485436893203883, 0.6679611650485436, 0.6466019417475728, 0.6504854368932039, 0.6155339805825243, 0.658252427184466, 0.6640776699029126, 0.6621359223300971]
Running Loop: 9/10
Epoch 1, Loss: 0.6103
Epoch 2, Loss: 0.6072
Epoch 3, Loss: 0.6043
Epoch 4, Loss: 0.5998
Epoch 5, Loss: 0.5923
Epoch 6, Loss: 0.5881
Epoch 7, Loss: 0.5690
Epoch 8, Loss: 0.5565
Epoch 9, Loss: 0.5413
Epoch 10, Loss: 0.5277
Test set: Average loss: 0.0115, Accuracy: 308/515 (59.81%)
accuracy= 0.5980582524271845
accuracies= [0.6485436893203883, 0.6679611650485436, 0.6466019417475728, 0.6504854368932039, 0.6155339805825243, 0.658252427184466, 0.6640776699029126, 0.6621359223300971, 0.5980582524271845]
Running Loop: 10/10
Epoch 1, Loss: 0.6109
Epoch 2, Loss: 0.6080
Epoch 3, Loss: 0.6068
Epoch 4, Loss: 0.6050
Epoch 5, Loss: 0.6016
Epoch 6, Loss: 0.6004
Epoch 7, Loss: 0.5899
Epoch 8, Loss: 0.5839
Epoch 9, Loss: 0.5630
Epoch 10, Loss: 0.5487
Test set: Average loss: 0.0112, Accuracy: 337/515 (65.44%)
accuracy= 0.654368932038835
accuracies= [0.6485436893203883, 0.6679611650485436, 0.6466019417475728, 0.6504854368932039, 0.6155339805825243, 0.658252427184466, 0.6640776699029126, 0.6621359223300971, 0.5980582524271845, 0.654368932038835]

Running CoLA Training with Optimizer = HeavyBall
params= {'lr': 0.001, 'momentum': 0.9}
Running Loop: 1/10
Epoch 1, Loss: 0.6232
Epoch 2, Loss: 0.6117
Epoch 3, Loss: 0.6107
Epoch 4, Loss: 0.6098
Epoch 5, Loss: 0.6095
Epoch 6, Loss: 0.6092
Epoch 7, Loss: 0.6091
Epoch 8, Loss: 0.6088
Epoch 9, Loss: 0.6084
Epoch 10, Loss: 0.6082
Test set: Average loss: 0.0109, Accuracy: 353/515 (68.54%)
accuracy= 0.6854368932038835
accuracies= [0.6854368932038835]
Running Loop: 2/10
Epoch 1, Loss: 0.6258
Epoch 2, Loss: 0.6110
Epoch 3, Loss: 0.6099
Epoch 4, Loss: 0.6092
Epoch 5, Loss: 0.6091
Epoch 6, Loss: 0.6088
Epoch 7, Loss: 0.6084
Epoch 8, Loss: 0.6082
Epoch 9, Loss: 0.6079
Epoch 10, Loss: 0.6079
Test set: Average loss: 0.0109, Accuracy: 353/515 (68.54%)
accuracy= 0.6854368932038835
accuracies= [0.6854368932038835, 0.6854368932038835]
Running Loop: 3/10
Epoch 1, Loss: 0.6243
Epoch 2, Loss: 0.6120
Epoch 3, Loss: 0.6106
Epoch 4, Loss: 0.6101
Epoch 5, Loss: 0.6093
Epoch 6, Loss: 0.6095
Epoch 7, Loss: 0.6090
Epoch 8, Loss: 0.6084
Epoch 9, Loss: 0.6084
Epoch 10, Loss: 0.6078
Test set: Average loss: 0.0109, Accuracy: 353/515 (68.54%)
accuracy= 0.6854368932038835
accuracies= [0.6854368932038835, 0.6854368932038835, 0.6854368932038835]
Running Loop: 4/10
Epoch 1, Loss: 0.6235
Epoch 2, Loss: 0.6098
Epoch 3, Loss: 0.6095
Epoch 4, Loss: 0.6089
Epoch 5, Loss: 0.6089
Epoch 6, Loss: 0.6084
Epoch 7, Loss: 0.6082
Epoch 8, Loss: 0.6078
Epoch 9, Loss: 0.6074
Epoch 10, Loss: 0.6074
Test set: Average loss: 0.0109, Accuracy: 353/515 (68.54%)
accuracy= 0.6854368932038835
accuracies= [0.6854368932038835, 0.6854368932038835, 0.6854368932038835, 0.6854368932038835]
Running Loop: 5/10
Epoch 1, Loss: 0.6258
Epoch 2, Loss: 0.6112
Epoch 3, Loss: 0.6101
Epoch 4, Loss: 0.6097
Epoch 5, Loss: 0.6092
Epoch 6, Loss: 0.6087
Epoch 7, Loss: 0.6086
Epoch 8, Loss: 0.6089
Epoch 9, Loss: 0.6081
Epoch 10, Loss: 0.6077
Test set: Average loss: 0.0109, Accuracy: 353/515 (68.54%)
accuracy= 0.6854368932038835
accuracies= [0.6854368932038835, 0.6854368932038835, 0.6854368932038835, 0.6854368932038835, 0.6854368932038835]
Running Loop: 6/10
Epoch 1, Loss: 0.6228
Epoch 2, Loss: 0.6108
Epoch 3, Loss: 0.6097
Epoch 4, Loss: 0.6094
Epoch 5, Loss: 0.6091
Epoch 6, Loss: 0.6090
Epoch 7, Loss: 0.6084
Epoch 8, Loss: 0.6082
Epoch 9, Loss: 0.6078
Epoch 10, Loss: 0.6080
Test set: Average loss: 0.0109, Accuracy: 353/515 (68.54%)
accuracy= 0.6854368932038835
accuracies= [0.6854368932038835, 0.6854368932038835, 0.6854368932038835, 0.6854368932038835, 0.6854368932038835, 0.6854368932038835]
Running Loop: 7/10
Epoch 1, Loss: 0.6250
Epoch 2, Loss: 0.6108
Epoch 3, Loss: 0.6099
Epoch 4, Loss: 0.6094
Epoch 5, Loss: 0.6091
Epoch 6, Loss: 0.6091
Epoch 7, Loss: 0.6084
Epoch 8, Loss: 0.6087
Epoch 9, Loss: 0.6081
Epoch 10, Loss: 0.6080
Test set: Average loss: 0.0109, Accuracy: 353/515 (68.54%)
accuracy= 0.6854368932038835
accuracies= [0.6854368932038835, 0.6854368932038835, 0.6854368932038835, 0.6854368932038835, 0.6854368932038835, 0.6854368932038835, 0.6854368932038835]
Running Loop: 8/10
Epoch 1, Loss: 0.6208
Epoch 2, Loss: 0.6104
Epoch 3, Loss: 0.6093
Epoch 4, Loss: 0.6091
Epoch 5, Loss: 0.6086
Epoch 6, Loss: 0.6084
Epoch 7, Loss: 0.6086
Epoch 8, Loss: 0.6079
Epoch 9, Loss: 0.6077
Epoch 10, Loss: 0.6080
Test set: Average loss: 0.0109, Accuracy: 353/515 (68.54%)
accuracy= 0.6854368932038835
accuracies= [0.6854368932038835, 0.6854368932038835, 0.6854368932038835, 0.6854368932038835, 0.6854368932038835, 0.6854368932038835, 0.6854368932038835, 0.6854368932038835]
Running Loop: 9/10
Epoch 1, Loss: 0.6223
Epoch 2, Loss: 0.6117
Epoch 3, Loss: 0.6110
Epoch 4, Loss: 0.6102
Epoch 5, Loss: 0.6097
Epoch 6, Loss: 0.6098
Epoch 7, Loss: 0.6091
Epoch 8, Loss: 0.6087
Epoch 9, Loss: 0.6085
Epoch 10, Loss: 0.6084
Test set: Average loss: 0.0109, Accuracy: 353/515 (68.54%)
accuracy= 0.6854368932038835
accuracies= [0.6854368932038835, 0.6854368932038835, 0.6854368932038835, 0.6854368932038835, 0.6854368932038835, 0.6854368932038835, 0.6854368932038835, 0.6854368932038835, 0.6854368932038835]
Running Loop: 10/10
Epoch 1, Loss: 0.6249
Epoch 2, Loss: 0.6114
Epoch 3, Loss: 0.6105
Epoch 4, Loss: 0.6099
Epoch 5, Loss: 0.6097
Epoch 6, Loss: 0.6093
Epoch 7, Loss: 0.6086
Epoch 8, Loss: 0.6084
Epoch 9, Loss: 0.6086
Epoch 10, Loss: 0.6081
Test set: Average loss: 0.0109, Accuracy: 353/515 (68.54%)
accuracy= 0.6854368932038835
accuracies= [0.6854368932038835, 0.6854368932038835, 0.6854368932038835, 0.6854368932038835, 0.6854368932038835, 0.6854368932038835, 0.6854368932038835, 0.6854368932038835, 0.6854368932038835, 0.6854368932038835]

Running CoLA Training with Optimizer = HeavyBallCurvature
params= {'lr': 0.001, 'momentum': 0.55, 'epsilon': 0.01}
Running Loop: 1/10
Epoch 1, Loss: 0.6112
Epoch 2, Loss: 0.6073
Epoch 3, Loss: 0.6053
Epoch 4, Loss: 0.6026
Epoch 5, Loss: 0.5988
Epoch 6, Loss: 0.5955
Epoch 7, Loss: 0.5916
Epoch 8, Loss: 0.5867
Epoch 9, Loss: 0.5798
Epoch 10, Loss: 0.5760
Test set: Average loss: 0.0110, Accuracy: 330/515 (64.08%)
accuracy= 0.6407766990291263
accuracies= [0.6407766990291263]
Running Loop: 2/10
Epoch 1, Loss: 0.6104
Epoch 2, Loss: 0.6060
Epoch 3, Loss: 0.6026
Epoch 4, Loss: 0.6003
Epoch 5, Loss: 0.5970
Epoch 6, Loss: 0.5927
Epoch 7, Loss: 0.5884
Epoch 8, Loss: 0.5831
Epoch 9, Loss: 0.5786
Epoch 10, Loss: 0.5764
Test set: Average loss: 0.0115, Accuracy: 342/515 (66.41%)
accuracy= 0.6640776699029126
accuracies= [0.6407766990291263, 0.6640776699029126]
Running Loop: 3/10
Epoch 1, Loss: 0.6107
Epoch 2, Loss: 0.6072
Epoch 3, Loss: 0.6050
Epoch 4, Loss: 0.6022
Epoch 5, Loss: 0.5977
Epoch 6, Loss: 0.5938
Epoch 7, Loss: 0.5882
Epoch 8, Loss: 0.5828
Epoch 9, Loss: 0.5787
Epoch 10, Loss: 0.5737
Test set: Average loss: 0.0120, Accuracy: 330/515 (64.08%)
accuracy= 0.6407766990291263
accuracies= [0.6407766990291263, 0.6640776699029126, 0.6407766990291263]
Running Loop: 4/10
Epoch 1, Loss: 0.6104
Epoch 2, Loss: 0.6066
Epoch 3, Loss: 0.6049
Epoch 4, Loss: 0.6013
Epoch 5, Loss: 0.5989
Epoch 6, Loss: 0.5944
Epoch 7, Loss: 0.5910
Epoch 8, Loss: 0.5873
Epoch 9, Loss: 0.5838
Epoch 10, Loss: 0.5802
Test set: Average loss: 0.0110, Accuracy: 347/515 (67.38%)
accuracy= 0.6737864077669903
accuracies= [0.6407766990291263, 0.6640776699029126, 0.6407766990291263, 0.6737864077669903]
Running Loop: 5/10
Epoch 1, Loss: 0.6105
Epoch 2, Loss: 0.6063
Epoch 3, Loss: 0.6030
Epoch 4, Loss: 0.6000
Epoch 5, Loss: 0.5960
Epoch 6, Loss: 0.5917
Epoch 7, Loss: 0.5868
Epoch 8, Loss: 0.5841
Epoch 9, Loss: 0.5779
Epoch 10, Loss: 0.5739
Test set: Average loss: 0.0106, Accuracy: 336/515 (65.24%)
accuracy= 0.6524271844660194
accuracies= [0.6407766990291263, 0.6640776699029126, 0.6407766990291263, 0.6737864077669903, 0.6524271844660194]
Running Loop: 6/10
Epoch 1, Loss: 0.6100
Epoch 2, Loss: 0.6051
Epoch 3, Loss: 0.6019
Epoch 4, Loss: 0.5976
Epoch 5, Loss: 0.5934
Epoch 6, Loss: 0.5897
Epoch 7, Loss: 0.5851
Epoch 8, Loss: 0.5803
Epoch 9, Loss: 0.5764
Epoch 10, Loss: 0.5722
Test set: Average loss: 0.0127, Accuracy: 319/515 (61.94%)
accuracy= 0.6194174757281553
accuracies= [0.6407766990291263, 0.6640776699029126, 0.6407766990291263, 0.6737864077669903, 0.6524271844660194, 0.6194174757281553]
Running Loop: 7/10
Epoch 1, Loss: 0.6097
Epoch 2, Loss: 0.6065
Epoch 3, Loss: 0.6044
Epoch 4, Loss: 0.6008
Epoch 5, Loss: 0.5972
Epoch 6, Loss: 0.5933
Epoch 7, Loss: 0.5885
Epoch 8, Loss: 0.5849
Epoch 9, Loss: 0.5800
Epoch 10, Loss: 0.5763
Test set: Average loss: 0.0107, Accuracy: 333/515 (64.66%)
accuracy= 0.6466019417475728
accuracies= [0.6407766990291263, 0.6640776699029126, 0.6407766990291263, 0.6737864077669903, 0.6524271844660194, 0.6194174757281553, 0.6466019417475728]
Running Loop: 8/10
Epoch 1, Loss: 0.6099
Epoch 2, Loss: 0.6067
Epoch 3, Loss: 0.6049
Epoch 4, Loss: 0.6031
Epoch 5, Loss: 0.5992
Epoch 6, Loss: 0.5965
Epoch 7, Loss: 0.5922
Epoch 8, Loss: 0.5879
Epoch 9, Loss: 0.5837
Epoch 10, Loss: 0.5783
Test set: Average loss: 0.0115, Accuracy: 325/515 (63.11%)
accuracy= 0.6310679611650486
accuracies= [0.6407766990291263, 0.6640776699029126, 0.6407766990291263, 0.6737864077669903, 0.6524271844660194, 0.6194174757281553, 0.6466019417475728, 0.6310679611650486]
Running Loop: 9/10
Epoch 1, Loss: 0.6103
Epoch 2, Loss: 0.6062
Epoch 3, Loss: 0.6037
Epoch 4, Loss: 0.6001
Epoch 5, Loss: 0.5961
Epoch 6, Loss: 0.5922
Epoch 7, Loss: 0.5890
Epoch 8, Loss: 0.5836
Epoch 9, Loss: 0.5804
Epoch 10, Loss: 0.5753
Test set: Average loss: 0.0112, Accuracy: 340/515 (66.02%)
accuracy= 0.6601941747572816
accuracies= [0.6407766990291263, 0.6640776699029126, 0.6407766990291263, 0.6737864077669903, 0.6524271844660194, 0.6194174757281553, 0.6466019417475728, 0.6310679611650486, 0.6601941747572816]
Running Loop: 10/10
Epoch 1, Loss: 0.6106
Epoch 2, Loss: 0.6065
Epoch 3, Loss: 0.6034
Epoch 4, Loss: 0.6007
Epoch 5, Loss: 0.5971
Epoch 6, Loss: 0.5941
Epoch 7, Loss: 0.5891
Epoch 8, Loss: 0.5838
Epoch 9, Loss: 0.5802
Epoch 10, Loss: 0.5764
Test set: Average loss: 0.0109, Accuracy: 354/515 (68.74%)
accuracy= 0.6873786407766991
accuracies= [0.6407766990291263, 0.6640776699029126, 0.6407766990291263, 0.6737864077669903, 0.6524271844660194, 0.6194174757281553, 0.6466019417475728, 0.6310679611650486, 0.6601941747572816, 0.6873786407766991]

Running CoLA Training with Optimizer = NAG
params= {'lr': 0.001, 'momentum': 0.9}
Running Loop: 1/10
Epoch 1, Loss: 0.6232
Epoch 2, Loss: 0.6117
Epoch 3, Loss: 0.6107
Epoch 4, Loss: 0.6098
Epoch 5, Loss: 0.6095
Epoch 6, Loss: 0.6092
Epoch 7, Loss: 0.6091
Epoch 8, Loss: 0.6088
Epoch 9, Loss: 0.6084
Epoch 10, Loss: 0.6082
Test set: Average loss: 0.0109, Accuracy: 353/515 (68.54%)
accuracy= 0.6854368932038835
accuracies= [0.6854368932038835]
Running Loop: 2/10
Epoch 1, Loss: 0.6258
Epoch 2, Loss: 0.6110
Epoch 3, Loss: 0.6099
Epoch 4, Loss: 0.6092
Epoch 5, Loss: 0.6091
Epoch 6, Loss: 0.6088
Epoch 7, Loss: 0.6084
Epoch 8, Loss: 0.6082
Epoch 9, Loss: 0.6079
Epoch 10, Loss: 0.6079
Test set: Average loss: 0.0109, Accuracy: 353/515 (68.54%)
accuracy= 0.6854368932038835
accuracies= [0.6854368932038835, 0.6854368932038835]
Running Loop: 3/10
Epoch 1, Loss: 0.6243
Epoch 2, Loss: 0.6120
Epoch 3, Loss: 0.6106
Epoch 4, Loss: 0.6101
Epoch 5, Loss: 0.6093
Epoch 6, Loss: 0.6095
Epoch 7, Loss: 0.6090
Epoch 8, Loss: 0.6084
Epoch 9, Loss: 0.6084
Epoch 10, Loss: 0.6078
Test set: Average loss: 0.0109, Accuracy: 353/515 (68.54%)
accuracy= 0.6854368932038835
accuracies= [0.6854368932038835, 0.6854368932038835, 0.6854368932038835]
Running Loop: 4/10
Epoch 1, Loss: 0.6235
Epoch 2, Loss: 0.6098
Epoch 3, Loss: 0.6095
Epoch 4, Loss: 0.6089
Epoch 5, Loss: 0.6089
Epoch 6, Loss: 0.6084
Epoch 7, Loss: 0.6082
Epoch 8, Loss: 0.6078
Epoch 9, Loss: 0.6074
Epoch 10, Loss: 0.6074
Test set: Average loss: 0.0109, Accuracy: 353/515 (68.54%)
accuracy= 0.6854368932038835
accuracies= [0.6854368932038835, 0.6854368932038835, 0.6854368932038835, 0.6854368932038835]
Running Loop: 5/10
Epoch 1, Loss: 0.6258
Epoch 2, Loss: 0.6112
Epoch 3, Loss: 0.6101
Epoch 4, Loss: 0.6097
Epoch 5, Loss: 0.6092
Epoch 6, Loss: 0.6087
Epoch 7, Loss: 0.6086
Epoch 8, Loss: 0.6089
Epoch 9, Loss: 0.6081
Epoch 10, Loss: 0.6077
Test set: Average loss: 0.0109, Accuracy: 353/515 (68.54%)
accuracy= 0.6854368932038835
accuracies= [0.6854368932038835, 0.6854368932038835, 0.6854368932038835, 0.6854368932038835, 0.6854368932038835]
Running Loop: 6/10
Epoch 1, Loss: 0.6228
Epoch 2, Loss: 0.6108
Epoch 3, Loss: 0.6097
Epoch 4, Loss: 0.6094
Epoch 5, Loss: 0.6091
Epoch 6, Loss: 0.6090
Epoch 7, Loss: 0.6084
Epoch 8, Loss: 0.6082
Epoch 9, Loss: 0.6078
Epoch 10, Loss: 0.6080
Test set: Average loss: 0.0109, Accuracy: 353/515 (68.54%)
accuracy= 0.6854368932038835
accuracies= [0.6854368932038835, 0.6854368932038835, 0.6854368932038835, 0.6854368932038835, 0.6854368932038835, 0.6854368932038835]
Running Loop: 7/10
Epoch 1, Loss: 0.6250
Epoch 2, Loss: 0.6108
Epoch 3, Loss: 0.6099
Epoch 4, Loss: 0.6094
Epoch 5, Loss: 0.6091
Epoch 6, Loss: 0.6091
Epoch 7, Loss: 0.6084
Epoch 8, Loss: 0.6087
Epoch 9, Loss: 0.6081
Epoch 10, Loss: 0.6080
Test set: Average loss: 0.0109, Accuracy: 353/515 (68.54%)
accuracy= 0.6854368932038835
accuracies= [0.6854368932038835, 0.6854368932038835, 0.6854368932038835, 0.6854368932038835, 0.6854368932038835, 0.6854368932038835, 0.6854368932038835]
Running Loop: 8/10
Epoch 1, Loss: 0.6208
Epoch 2, Loss: 0.6104
Epoch 3, Loss: 0.6093
Epoch 4, Loss: 0.6091
Epoch 5, Loss: 0.6086
Epoch 6, Loss: 0.6084
Epoch 7, Loss: 0.6086
Epoch 8, Loss: 0.6079
Epoch 9, Loss: 0.6077
Epoch 10, Loss: 0.6080
Test set: Average loss: 0.0109, Accuracy: 353/515 (68.54%)
accuracy= 0.6854368932038835
accuracies= [0.6854368932038835, 0.6854368932038835, 0.6854368932038835, 0.6854368932038835, 0.6854368932038835, 0.6854368932038835, 0.6854368932038835, 0.6854368932038835]
Running Loop: 9/10
Epoch 1, Loss: 0.6223
Epoch 2, Loss: 0.6117
Epoch 3, Loss: 0.6110
Epoch 4, Loss: 0.6102
Epoch 5, Loss: 0.6097
Epoch 6, Loss: 0.6098
Epoch 7, Loss: 0.6091
Epoch 8, Loss: 0.6087
Epoch 9, Loss: 0.6085
Epoch 10, Loss: 0.6084
Test set: Average loss: 0.0109, Accuracy: 353/515 (68.54%)
accuracy= 0.6854368932038835
accuracies= [0.6854368932038835, 0.6854368932038835, 0.6854368932038835, 0.6854368932038835, 0.6854368932038835, 0.6854368932038835, 0.6854368932038835, 0.6854368932038835, 0.6854368932038835]
Running Loop: 10/10
Epoch 1, Loss: 0.6249
Epoch 2, Loss: 0.6114
Epoch 3, Loss: 0.6105
Epoch 4, Loss: 0.6099
Epoch 5, Loss: 0.6097
Epoch 6, Loss: 0.6093
Epoch 7, Loss: 0.6086
Epoch 8, Loss: 0.6084
Epoch 9, Loss: 0.6086
Epoch 10, Loss: 0.6081
Test set: Average loss: 0.0109, Accuracy: 353/515 (68.54%)
accuracy= 0.6854368932038835
accuracies= [0.6854368932038835, 0.6854368932038835, 0.6854368932038835, 0.6854368932038835, 0.6854368932038835, 0.6854368932038835, 0.6854368932038835, 0.6854368932038835, 0.6854368932038835, 0.6854368932038835]

Running CoLA Training with Optimizer = NAGCurvature
params= {'lr': 0.001, 'momentum': 0.55, 'epsilon': 0.01}
Running Loop: 1/10
Epoch 1, Loss: 0.6112
Epoch 2, Loss: 0.6073
Epoch 3, Loss: 0.6053
Epoch 4, Loss: 0.6026
Epoch 5, Loss: 0.5988
Epoch 6, Loss: 0.5955
Epoch 7, Loss: 0.5916
Epoch 8, Loss: 0.5867
Epoch 9, Loss: 0.5798
Epoch 10, Loss: 0.5760
Test set: Average loss: 0.0110, Accuracy: 330/515 (64.08%)
accuracy= 0.6407766990291263
accuracies= [0.6407766990291263]
Running Loop: 2/10
Epoch 1, Loss: 0.6104
Epoch 2, Loss: 0.6060
Epoch 3, Loss: 0.6026
Epoch 4, Loss: 0.6003
Epoch 5, Loss: 0.5970
Epoch 6, Loss: 0.5927
Epoch 7, Loss: 0.5884
Epoch 8, Loss: 0.5831
Epoch 9, Loss: 0.5786
Epoch 10, Loss: 0.5764
Test set: Average loss: 0.0115, Accuracy: 342/515 (66.41%)
accuracy= 0.6640776699029126
accuracies= [0.6407766990291263, 0.6640776699029126]
Running Loop: 3/10
Epoch 1, Loss: 0.6107
Epoch 2, Loss: 0.6072
Epoch 3, Loss: 0.6050
Epoch 4, Loss: 0.6022
Epoch 5, Loss: 0.5977
Epoch 6, Loss: 0.5938
Epoch 7, Loss: 0.5882
Epoch 8, Loss: 0.5828
Epoch 9, Loss: 0.5787
Epoch 10, Loss: 0.5737
Test set: Average loss: 0.0120, Accuracy: 330/515 (64.08%)
accuracy= 0.6407766990291263
accuracies= [0.6407766990291263, 0.6640776699029126, 0.6407766990291263]
Running Loop: 4/10
Epoch 1, Loss: 0.6104
Epoch 2, Loss: 0.6066
Epoch 3, Loss: 0.6049
Epoch 4, Loss: 0.6013
Epoch 5, Loss: 0.5989
Epoch 6, Loss: 0.5944
Epoch 7, Loss: 0.5910
Epoch 8, Loss: 0.5873
Epoch 9, Loss: 0.5838
Epoch 10, Loss: 0.5802
Test set: Average loss: 0.0110, Accuracy: 347/515 (67.38%)
accuracy= 0.6737864077669903
accuracies= [0.6407766990291263, 0.6640776699029126, 0.6407766990291263, 0.6737864077669903]
Running Loop: 5/10
Epoch 1, Loss: 0.6105
Epoch 2, Loss: 0.6063
Epoch 3, Loss: 0.6030
Epoch 4, Loss: 0.6000
Epoch 5, Loss: 0.5960
Epoch 6, Loss: 0.5917
Epoch 7, Loss: 0.5868
Epoch 8, Loss: 0.5841
Epoch 9, Loss: 0.5779
Epoch 10, Loss: 0.5739
Test set: Average loss: 0.0106, Accuracy: 336/515 (65.24%)
accuracy= 0.6524271844660194
accuracies= [0.6407766990291263, 0.6640776699029126, 0.6407766990291263, 0.6737864077669903, 0.6524271844660194]
Running Loop: 6/10
Epoch 1, Loss: 0.6100
Epoch 2, Loss: 0.6051
Epoch 3, Loss: 0.6019
Epoch 4, Loss: 0.5976
Epoch 5, Loss: 0.5934
Epoch 6, Loss: 0.5897
Epoch 7, Loss: 0.5851
Epoch 8, Loss: 0.5803
Epoch 9, Loss: 0.5764
Epoch 10, Loss: 0.5722
Test set: Average loss: 0.0127, Accuracy: 319/515 (61.94%)
accuracy= 0.6194174757281553
accuracies= [0.6407766990291263, 0.6640776699029126, 0.6407766990291263, 0.6737864077669903, 0.6524271844660194, 0.6194174757281553]
Running Loop: 7/10
Epoch 1, Loss: 0.6097
Epoch 2, Loss: 0.6065
Epoch 3, Loss: 0.6044
Epoch 4, Loss: 0.6008
Epoch 5, Loss: 0.5972
Epoch 6, Loss: 0.5933
Epoch 7, Loss: 0.5885
Epoch 8, Loss: 0.5849
Epoch 9, Loss: 0.5800
Epoch 10, Loss: 0.5763
Test set: Average loss: 0.0107, Accuracy: 333/515 (64.66%)
accuracy= 0.6466019417475728
accuracies= [0.6407766990291263, 0.6640776699029126, 0.6407766990291263, 0.6737864077669903, 0.6524271844660194, 0.6194174757281553, 0.6466019417475728]
Running Loop: 8/10
Epoch 1, Loss: 0.6099
Epoch 2, Loss: 0.6067
Epoch 3, Loss: 0.6049
Epoch 4, Loss: 0.6031
Epoch 5, Loss: 0.5992
Epoch 6, Loss: 0.5965
Epoch 7, Loss: 0.5922
Epoch 8, Loss: 0.5879
Epoch 9, Loss: 0.5837
Epoch 10, Loss: 0.5783
Test set: Average loss: 0.0115, Accuracy: 325/515 (63.11%)
accuracy= 0.6310679611650486
accuracies= [0.6407766990291263, 0.6640776699029126, 0.6407766990291263, 0.6737864077669903, 0.6524271844660194, 0.6194174757281553, 0.6466019417475728, 0.6310679611650486]
Running Loop: 9/10
Epoch 1, Loss: 0.6103
Epoch 2, Loss: 0.6062
Epoch 3, Loss: 0.6037
Epoch 4, Loss: 0.6001
Epoch 5, Loss: 0.5961
Epoch 6, Loss: 0.5922
Epoch 7, Loss: 0.5890
Epoch 8, Loss: 0.5836
Epoch 9, Loss: 0.5804
Epoch 10, Loss: 0.5753
Test set: Average loss: 0.0112, Accuracy: 340/515 (66.02%)
accuracy= 0.6601941747572816
accuracies= [0.6407766990291263, 0.6640776699029126, 0.6407766990291263, 0.6737864077669903, 0.6524271844660194, 0.6194174757281553, 0.6466019417475728, 0.6310679611650486, 0.6601941747572816]
Running Loop: 10/10
Epoch 1, Loss: 0.6106
Epoch 2, Loss: 0.6065
Epoch 3, Loss: 0.6034
Epoch 4, Loss: 0.6007
Epoch 5, Loss: 0.5971
Epoch 6, Loss: 0.5941
Epoch 7, Loss: 0.5891
Epoch 8, Loss: 0.5838
Epoch 9, Loss: 0.5802
Epoch 10, Loss: 0.5764
Test set: Average loss: 0.0109, Accuracy: 354/515 (68.74%)
accuracy= 0.6873786407766991
accuracies= [0.6407766990291263, 0.6640776699029126, 0.6407766990291263, 0.6737864077669903, 0.6524271844660194, 0.6194174757281553, 0.6466019417475728, 0.6310679611650486, 0.6601941747572816, 0.6873786407766991]

Running CoLA Training with Optimizer = Adagrad
params= {'lr': 0.01, 'eps': 1e-10}
Running Loop: 1/10
Epoch 1, Loss: 0.6145
Epoch 2, Loss: 0.6081
Epoch 3, Loss: 0.6047
Epoch 4, Loss: 0.6024
Epoch 5, Loss: 0.5942
Epoch 6, Loss: 0.5893
Epoch 7, Loss: 0.5790
Epoch 8, Loss: 0.5651
Epoch 9, Loss: 0.5515
Epoch 10, Loss: 0.5384
Test set: Average loss: 0.0110, Accuracy: 343/515 (66.60%)
accuracy= 0.6660194174757281
accuracies= [0.6660194174757281]
Running Loop: 2/10
Epoch 1, Loss: 0.6113
Epoch 2, Loss: 0.6062
Epoch 3, Loss: 0.6004
Epoch 4, Loss: 0.5908
Epoch 5, Loss: 0.5794
Epoch 6, Loss: 0.5635
Epoch 7, Loss: 0.5470
Epoch 8, Loss: 0.5314
Epoch 9, Loss: 0.5187
Epoch 10, Loss: 0.5033
Test set: Average loss: 0.0112, Accuracy: 330/515 (64.08%)
accuracy= 0.6407766990291263
accuracies= [0.6660194174757281, 0.6407766990291263]
Running Loop: 3/10
Epoch 1, Loss: 0.6108
Epoch 2, Loss: 0.6080
Epoch 3, Loss: 0.6070
Epoch 4, Loss: 0.6058
Epoch 5, Loss: 0.6023
Epoch 6, Loss: 0.5991
Epoch 7, Loss: 0.5957
Epoch 8, Loss: 0.5886
Epoch 9, Loss: 0.5777
Epoch 10, Loss: 0.5643
Test set: Average loss: 0.0123, Accuracy: 341/515 (66.21%)
accuracy= 0.6621359223300971
accuracies= [0.6660194174757281, 0.6407766990291263, 0.6621359223300971]
Running Loop: 4/10
Epoch 1, Loss: 0.6113
Epoch 2, Loss: 0.6085
Epoch 3, Loss: 0.6067
Epoch 4, Loss: 0.6025
Epoch 5, Loss: 0.5981
Epoch 6, Loss: 0.5876
Epoch 7, Loss: 0.5741
Epoch 8, Loss: 0.5615
Epoch 9, Loss: 0.5472
Epoch 10, Loss: 0.5367
Test set: Average loss: 0.0107, Accuracy: 345/515 (66.99%)
accuracy= 0.6699029126213593
accuracies= [0.6660194174757281, 0.6407766990291263, 0.6621359223300971, 0.6699029126213593]
Running Loop: 5/10
Epoch 1, Loss: 0.6121
Epoch 2, Loss: 0.6078
Epoch 3, Loss: 0.6070
Epoch 4, Loss: 0.6026
Epoch 5, Loss: 0.5973
Epoch 6, Loss: 0.5876
Epoch 7, Loss: 0.5767
Epoch 8, Loss: 0.5648
Epoch 9, Loss: 0.5490
Epoch 10, Loss: 0.5373
Test set: Average loss: 0.0107, Accuracy: 336/515 (65.24%)
accuracy= 0.6524271844660194
accuracies= [0.6660194174757281, 0.6407766990291263, 0.6621359223300971, 0.6699029126213593, 0.6524271844660194]
Running Loop: 6/10
Epoch 1, Loss: 0.6110
Epoch 2, Loss: 0.6066
Epoch 3, Loss: 0.6004
Epoch 4, Loss: 0.5911
Epoch 5, Loss: 0.5788
Epoch 6, Loss: 0.5632
Epoch 7, Loss: 0.5481
Epoch 8, Loss: 0.5347
Epoch 9, Loss: 0.5238
Epoch 10, Loss: 0.5112
Test set: Average loss: 0.0114, Accuracy: 322/515 (62.52%)
accuracy= 0.625242718446602
accuracies= [0.6660194174757281, 0.6407766990291263, 0.6621359223300971, 0.6699029126213593, 0.6524271844660194, 0.625242718446602]
Running Loop: 7/10
Epoch 1, Loss: 0.6115
Epoch 2, Loss: 0.6047
Epoch 3, Loss: 0.5998
Epoch 4, Loss: 0.5889
Epoch 5, Loss: 0.5761
Epoch 6, Loss: 0.5597
Epoch 7, Loss: 0.5418
Epoch 8, Loss: 0.5268
Epoch 9, Loss: 0.5109
Epoch 10, Loss: 0.4954
Test set: Average loss: 0.0107, Accuracy: 338/515 (65.63%)
accuracy= 0.6563106796116505
accuracies= [0.6660194174757281, 0.6407766990291263, 0.6621359223300971, 0.6699029126213593, 0.6524271844660194, 0.625242718446602, 0.6563106796116505]
Running Loop: 8/10
Epoch 1, Loss: 0.6110
Epoch 2, Loss: 0.6085
Epoch 3, Loss: 0.6054
Epoch 4, Loss: 0.5995
Epoch 5, Loss: 0.5903
Epoch 6, Loss: 0.5760
Epoch 7, Loss: 0.5641
Epoch 8, Loss: 0.5489
Epoch 9, Loss: 0.5361
Epoch 10, Loss: 0.5235
Test set: Average loss: 0.0117, Accuracy: 333/515 (64.66%)
accuracy= 0.6466019417475728
accuracies= [0.6660194174757281, 0.6407766990291263, 0.6621359223300971, 0.6699029126213593, 0.6524271844660194, 0.625242718446602, 0.6563106796116505, 0.6466019417475728]
Running Loop: 9/10
Epoch 1, Loss: 0.6106
Epoch 2, Loss: 0.6071
Epoch 3, Loss: 0.6023
Epoch 4, Loss: 0.6003
Epoch 5, Loss: 0.5894
Epoch 6, Loss: 0.5796
Epoch 7, Loss: 0.5685
Epoch 8, Loss: 0.5549
Epoch 9, Loss: 0.5442
Epoch 10, Loss: 0.5299
Test set: Average loss: 0.0107, Accuracy: 339/515 (65.83%)
accuracy= 0.658252427184466
accuracies= [0.6660194174757281, 0.6407766990291263, 0.6621359223300971, 0.6699029126213593, 0.6524271844660194, 0.625242718446602, 0.6563106796116505, 0.6466019417475728, 0.658252427184466]
Running Loop: 10/10
Epoch 1, Loss: 0.6101
Epoch 2, Loss: 0.6164
Epoch 3, Loss: 0.6081
Epoch 4, Loss: 0.6059
Epoch 5, Loss: 0.6013
Epoch 6, Loss: 0.5930
Epoch 7, Loss: 0.5829
Epoch 8, Loss: 0.5699
Epoch 9, Loss: 0.5577
Epoch 10, Loss: 0.5445
Test set: Average loss: 0.0113, Accuracy: 344/515 (66.80%)
accuracy= 0.6679611650485436
accuracies= [0.6660194174757281, 0.6407766990291263, 0.6621359223300971, 0.6699029126213593, 0.6524271844660194, 0.625242718446602, 0.6563106796116505, 0.6466019417475728, 0.658252427184466, 0.6679611650485436]

Running CoLA Training with Optimizer = Adadelta
params= {'lr': 1.0, 'rho': 0.95, 'eps': 1e-06}
Running Loop: 1/10
Epoch 1, Loss: 0.6103
Epoch 2, Loss: 0.6064
Epoch 3, Loss: 0.6031
Epoch 4, Loss: 0.6001
Epoch 5, Loss: 0.5945
Epoch 6, Loss: 0.5908
Epoch 7, Loss: 0.5846
Epoch 8, Loss: 0.5808
Epoch 9, Loss: 0.5732
Epoch 10, Loss: 0.5701
Test set: Average loss: 0.0117, Accuracy: 343/515 (66.60%)
accuracy= 0.6660194174757281
accuracies= [0.6660194174757281]
Running Loop: 2/10
Epoch 1, Loss: 0.6099
Epoch 2, Loss: 0.6046
Epoch 3, Loss: 0.5993
Epoch 4, Loss: 0.5961
Epoch 5, Loss: 0.5924
Epoch 6, Loss: 0.5887
Epoch 7, Loss: 0.5833
Epoch 8, Loss: 0.5791
Epoch 9, Loss: 0.5750
Epoch 10, Loss: 0.5669
Test set: Average loss: 0.0108, Accuracy: 333/515 (64.66%)
accuracy= 0.6466019417475728
accuracies= [0.6660194174757281, 0.6466019417475728]
Running Loop: 3/10
Epoch 1, Loss: 0.6098
Epoch 2, Loss: 0.6058
Epoch 3, Loss: 0.6014
Epoch 4, Loss: 0.5957
Epoch 5, Loss: 0.5915
Epoch 6, Loss: 0.5869
Epoch 7, Loss: 0.5839
Epoch 8, Loss: 0.5798
Epoch 9, Loss: 0.5756
Epoch 10, Loss: 0.5737
Test set: Average loss: 0.0113, Accuracy: 342/515 (66.41%)
accuracy= 0.6640776699029126
accuracies= [0.6660194174757281, 0.6466019417475728, 0.6640776699029126]
Running Loop: 4/10
Epoch 1, Loss: 0.6096
Epoch 2, Loss: 0.6053
Epoch 3, Loss: 0.6017
Epoch 4, Loss: 0.5960
Epoch 5, Loss: 0.5947
Epoch 6, Loss: 0.5892
Epoch 7, Loss: 0.5883
Epoch 8, Loss: 0.5842
Epoch 9, Loss: 0.5781
Epoch 10, Loss: 0.5745
Test set: Average loss: 0.0114, Accuracy: 332/515 (64.47%)
accuracy= 0.6446601941747573
accuracies= [0.6660194174757281, 0.6466019417475728, 0.6640776699029126, 0.6446601941747573]
Running Loop: 5/10
Epoch 1, Loss: 0.6101
Epoch 2, Loss: 0.6050
Epoch 3, Loss: 0.6011
Epoch 4, Loss: 0.5951
Epoch 5, Loss: 0.5921
Epoch 6, Loss: 0.5869
Epoch 7, Loss: 0.5842
Epoch 8, Loss: 0.5787
Epoch 9, Loss: 0.5774
Epoch 10, Loss: 0.5711
Test set: Average loss: 0.0111, Accuracy: 347/515 (67.38%)
accuracy= 0.6737864077669903
accuracies= [0.6660194174757281, 0.6466019417475728, 0.6640776699029126, 0.6446601941747573, 0.6737864077669903]
Running Loop: 6/10
Epoch 1, Loss: 0.6085
Epoch 2, Loss: 0.6040
Epoch 3, Loss: 0.5991
Epoch 4, Loss: 0.5958
Epoch 5, Loss: 0.5911
Epoch 6, Loss: 0.5857
Epoch 7, Loss: 0.5821
Epoch 8, Loss: 0.5773
Epoch 9, Loss: 0.5710
Epoch 10, Loss: 0.5689
Test set: Average loss: 0.0112, Accuracy: 335/515 (65.05%)
accuracy= 0.6504854368932039
accuracies= [0.6660194174757281, 0.6466019417475728, 0.6640776699029126, 0.6446601941747573, 0.6737864077669903, 0.6504854368932039]
Running Loop: 7/10
Epoch 1, Loss: 0.6092
Epoch 2, Loss: 0.6063
Epoch 3, Loss: 0.6021
Epoch 4, Loss: 0.5981
Epoch 5, Loss: 0.5931
Epoch 6, Loss: 0.5880
Epoch 7, Loss: 0.5827
Epoch 8, Loss: 0.5799
Epoch 9, Loss: 0.5736
Epoch 10, Loss: 0.5682
Test set: Average loss: 0.0110, Accuracy: 347/515 (67.38%)
accuracy= 0.6737864077669903
accuracies= [0.6660194174757281, 0.6466019417475728, 0.6640776699029126, 0.6446601941747573, 0.6737864077669903, 0.6504854368932039, 0.6737864077669903]
Running Loop: 8/10
Epoch 1, Loss: 0.6094
Epoch 2, Loss: 0.6064
Epoch 3, Loss: 0.6032
Epoch 4, Loss: 0.5982
Epoch 5, Loss: 0.5939
Epoch 6, Loss: 0.5885
Epoch 7, Loss: 0.5871
Epoch 8, Loss: 0.5805
Epoch 9, Loss: 0.5775
Epoch 10, Loss: 0.5742
Test set: Average loss: 0.0114, Accuracy: 335/515 (65.05%)
accuracy= 0.6504854368932039
accuracies= [0.6660194174757281, 0.6466019417475728, 0.6640776699029126, 0.6446601941747573, 0.6737864077669903, 0.6504854368932039, 0.6737864077669903, 0.6504854368932039]
Running Loop: 9/10
Epoch 1, Loss: 0.6094
Epoch 2, Loss: 0.6050
Epoch 3, Loss: 0.6011
Epoch 4, Loss: 0.5950
Epoch 5, Loss: 0.5933
Epoch 6, Loss: 0.5886
Epoch 7, Loss: 0.5840
Epoch 8, Loss: 0.5788
Epoch 9, Loss: 0.5761
Epoch 10, Loss: 0.5717
Test set: Average loss: 0.0114, Accuracy: 337/515 (65.44%)
accuracy= 0.654368932038835
accuracies= [0.6660194174757281, 0.6466019417475728, 0.6640776699029126, 0.6446601941747573, 0.6737864077669903, 0.6504854368932039, 0.6737864077669903, 0.6504854368932039, 0.654368932038835]
Running Loop: 10/10
Epoch 1, Loss: 0.6100
Epoch 2, Loss: 0.6046
Epoch 3, Loss: 0.6015
Epoch 4, Loss: 0.5972
Epoch 5, Loss: 0.5934
Epoch 6, Loss: 0.5900
Epoch 7, Loss: 0.5831
Epoch 8, Loss: 0.5772
Epoch 9, Loss: 0.5721
Epoch 10, Loss: 0.5696
Test set: Average loss: 0.0110, Accuracy: 347/515 (67.38%)
accuracy= 0.6737864077669903
accuracies= [0.6660194174757281, 0.6466019417475728, 0.6640776699029126, 0.6446601941747573, 0.6737864077669903, 0.6504854368932039, 0.6737864077669903, 0.6504854368932039, 0.654368932038835, 0.6737864077669903]

Running CoLA Training with Optimizer = RMSProp
params= {'lr': 0.01, 'alpha': 0.9, 'eps': 1e-08, 'weight_decay': 0}
Running Loop: 1/10
Epoch 1, Loss: 0.6242
Epoch 2, Loss: 0.6187
Epoch 3, Loss: 0.6158
Epoch 4, Loss: 0.6147
Epoch 5, Loss: 0.6127
Epoch 6, Loss: 0.6113
Epoch 7, Loss: 0.6095
Epoch 8, Loss: 0.6087
Epoch 9, Loss: 0.6079
Epoch 10, Loss: 0.6088
Test set: Average loss: 0.0110, Accuracy: 353/515 (68.54%)
accuracy= 0.6854368932038835
accuracies= [0.6854368932038835]
Running Loop: 2/10
Epoch 1, Loss: 0.6290
Epoch 2, Loss: 0.6155
Epoch 3, Loss: 0.6252
Epoch 4, Loss: 0.6119
Epoch 5, Loss: 0.6186
Epoch 6, Loss: 0.6086
Epoch 7, Loss: 0.6075
Epoch 8, Loss: 0.6072
Epoch 9, Loss: 0.6078
Epoch 10, Loss: 0.6072
Test set: Average loss: 0.0109, Accuracy: 355/515 (68.93%)
accuracy= 0.6893203883495146
accuracies= [0.6854368932038835, 0.6893203883495146]
Running Loop: 3/10
Epoch 1, Loss: 0.6150
Epoch 2, Loss: 0.6167
Epoch 3, Loss: 0.6112
Epoch 4, Loss: 0.6111
Epoch 5, Loss: 0.6086
Epoch 6, Loss: 0.6091
Epoch 7, Loss: 0.6129
Epoch 8, Loss: 0.6084
Epoch 9, Loss: 0.6092
Epoch 10, Loss: 0.6081
Test set: Average loss: 0.0110, Accuracy: 353/515 (68.54%)
accuracy= 0.6854368932038835
accuracies= [0.6854368932038835, 0.6893203883495146, 0.6854368932038835]
Running Loop: 4/10
Epoch 1, Loss: 0.6227
Epoch 2, Loss: 0.6147
Epoch 3, Loss: 0.6095
Epoch 4, Loss: 0.6089
Epoch 5, Loss: 0.6102
Epoch 6, Loss: 0.6086
Epoch 7, Loss: 0.6090
Epoch 8, Loss: 0.6081
Epoch 9, Loss: 0.6078
Epoch 10, Loss: 0.6095
Test set: Average loss: 0.0110, Accuracy: 353/515 (68.54%)
accuracy= 0.6854368932038835
accuracies= [0.6854368932038835, 0.6893203883495146, 0.6854368932038835, 0.6854368932038835]
Running Loop: 5/10
Epoch 1, Loss: 0.6442
Epoch 2, Loss: 0.6334
Epoch 3, Loss: 0.6133
Epoch 4, Loss: 0.6112
Epoch 5, Loss: 0.6127
Epoch 6, Loss: 0.6177
Epoch 7, Loss: 0.6100
Epoch 8, Loss: 0.6094
Epoch 9, Loss: 0.6092
Epoch 10, Loss: 0.6077
Test set: Average loss: 0.0110, Accuracy: 353/515 (68.54%)
accuracy= 0.6854368932038835
accuracies= [0.6854368932038835, 0.6893203883495146, 0.6854368932038835, 0.6854368932038835, 0.6854368932038835]
Running Loop: 6/10
Epoch 1, Loss: 0.6458
Epoch 2, Loss: 0.6158
Epoch 3, Loss: 0.6124
Epoch 4, Loss: 0.6095
Epoch 5, Loss: 0.6093
Epoch 6, Loss: 0.6103
Epoch 7, Loss: 0.6069
Epoch 8, Loss: 0.6086
Epoch 9, Loss: 0.6066
Epoch 10, Loss: 0.6067
Test set: Average loss: 0.0110, Accuracy: 353/515 (68.54%)
accuracy= 0.6854368932038835
accuracies= [0.6854368932038835, 0.6893203883495146, 0.6854368932038835, 0.6854368932038835, 0.6854368932038835, 0.6854368932038835]
Running Loop: 7/10
Epoch 1, Loss: 0.6245
Epoch 2, Loss: 0.6142
Epoch 3, Loss: 0.6214
Epoch 4, Loss: 0.6114
Epoch 5, Loss: 0.6112
Epoch 6, Loss: 0.6127
Epoch 7, Loss: 0.6097
Epoch 8, Loss: 0.6100
Epoch 9, Loss: 0.6092
Epoch 10, Loss: 0.6072
Test set: Average loss: 0.0110, Accuracy: 353/515 (68.54%)
accuracy= 0.6854368932038835
accuracies= [0.6854368932038835, 0.6893203883495146, 0.6854368932038835, 0.6854368932038835, 0.6854368932038835, 0.6854368932038835, 0.6854368932038835]
Running Loop: 8/10
Epoch 1, Loss: 0.6237
Epoch 2, Loss: 0.6145
Epoch 3, Loss: 0.6083
Epoch 4, Loss: 0.6087
Epoch 5, Loss: 0.6084
Epoch 6, Loss: 0.6086
Epoch 7, Loss: 0.6095
Epoch 8, Loss: 0.6081
Epoch 9, Loss: 0.6087
Epoch 10, Loss: 0.6145
Test set: Average loss: 0.0110, Accuracy: 353/515 (68.54%)
accuracy= 0.6854368932038835
accuracies= [0.6854368932038835, 0.6893203883495146, 0.6854368932038835, 0.6854368932038835, 0.6854368932038835, 0.6854368932038835, 0.6854368932038835, 0.6854368932038835]
Running Loop: 9/10
Epoch 1, Loss: 0.6225
Epoch 2, Loss: 0.6105
Epoch 3, Loss: 0.6100
Epoch 4, Loss: 0.6100
Epoch 5, Loss: 0.6085
Epoch 6, Loss: 0.6086
Epoch 7, Loss: 0.6084
Epoch 8, Loss: 0.6085
Epoch 9, Loss: 0.6086
Epoch 10, Loss: 0.6075
Test set: Average loss: 0.0110, Accuracy: 353/515 (68.54%)
accuracy= 0.6854368932038835
accuracies= [0.6854368932038835, 0.6893203883495146, 0.6854368932038835, 0.6854368932038835, 0.6854368932038835, 0.6854368932038835, 0.6854368932038835, 0.6854368932038835, 0.6854368932038835]
Running Loop: 10/10
Epoch 1, Loss: 0.6901
Epoch 2, Loss: 0.6119
Epoch 3, Loss: 0.6097
Epoch 4, Loss: 0.6089
Epoch 5, Loss: 0.6096
Epoch 6, Loss: 0.6078
Epoch 7, Loss: 0.6106
Epoch 8, Loss: 0.6071
Epoch 9, Loss: 0.6079
Epoch 10, Loss: 0.6074
Test set: Average loss: 0.0110, Accuracy: 353/515 (68.54%)
accuracy= 0.6854368932038835
accuracies= [0.6854368932038835, 0.6893203883495146, 0.6854368932038835, 0.6854368932038835, 0.6854368932038835, 0.6854368932038835, 0.6854368932038835, 0.6854368932038835, 0.6854368932038835, 0.6854368932038835]

Running CoLA Training with Optimizer = RMSPropMomentum
params= {'lr': 0.01, 'alpha': 0.9, 'eps': 1e-08, 'weight_decay': 0, 'momentum': 0.05}
Running Loop: 1/10
Epoch 1, Loss: 0.6243
Epoch 2, Loss: 0.6146
Epoch 3, Loss: 0.6097
Epoch 4, Loss: 0.6093
Epoch 5, Loss: 0.6093
Epoch 6, Loss: 0.6112
Epoch 7, Loss: 0.6107
Epoch 8, Loss: 0.6096
Epoch 9, Loss: 0.6082
Epoch 10, Loss: 0.6076
Test set: Average loss: 0.0110, Accuracy: 353/515 (68.54%)
accuracy= 0.6854368932038835
accuracies= [0.6854368932038835]
Running Loop: 2/10
Epoch 1, Loss: 0.6386
Epoch 2, Loss: 0.6126
Epoch 3, Loss: 0.6094
Epoch 4, Loss: 0.6108
Epoch 5, Loss: 0.6102
Epoch 6, Loss: 0.6086
Epoch 7, Loss: 0.6082
Epoch 8, Loss: 0.6076
Epoch 9, Loss: 0.6070
Epoch 10, Loss: 0.6059
Test set: Average loss: 0.0110, Accuracy: 353/515 (68.54%)
accuracy= 0.6854368932038835
accuracies= [0.6854368932038835, 0.6854368932038835]
Running Loop: 3/10
Epoch 1, Loss: 0.6250
Epoch 2, Loss: 0.6100
Epoch 3, Loss: 0.6089
Epoch 4, Loss: 0.6116
Epoch 5, Loss: 0.6082
Epoch 6, Loss: 0.6084
Epoch 7, Loss: 0.6130
Epoch 8, Loss: 0.6081
Epoch 9, Loss: 0.6078
Epoch 10, Loss: 0.6071
Test set: Average loss: 0.0110, Accuracy: 353/515 (68.54%)
accuracy= 0.6854368932038835
accuracies= [0.6854368932038835, 0.6854368932038835, 0.6854368932038835]
Running Loop: 4/10
Epoch 1, Loss: 0.6234
Epoch 2, Loss: 0.6160
Epoch 3, Loss: 0.6123
Epoch 4, Loss: 0.6107
Epoch 5, Loss: 0.6100
Epoch 6, Loss: 0.6084
Epoch 7, Loss: 0.6085
Epoch 8, Loss: 0.6081
Epoch 9, Loss: 0.6078
Epoch 10, Loss: 0.6084
Test set: Average loss: 0.0110, Accuracy: 353/515 (68.54%)
accuracy= 0.6854368932038835
accuracies= [0.6854368932038835, 0.6854368932038835, 0.6854368932038835, 0.6854368932038835]
Running Loop: 5/10
Epoch 1, Loss: 0.6802
Epoch 2, Loss: 0.7462
Epoch 3, Loss: 0.6340
Epoch 4, Loss: 0.6247
Epoch 5, Loss: 0.6117
Epoch 6, Loss: 0.6097
Epoch 7, Loss: 0.6095
Epoch 8, Loss: 0.6093
Epoch 9, Loss: 0.6074
Epoch 10, Loss: 0.6071
Test set: Average loss: 0.0110, Accuracy: 353/515 (68.54%)
accuracy= 0.6854368932038835
accuracies= [0.6854368932038835, 0.6854368932038835, 0.6854368932038835, 0.6854368932038835, 0.6854368932038835]
Running Loop: 6/10
Epoch 1, Loss: 0.6771
Epoch 2, Loss: 0.6141
Epoch 3, Loss: 0.6160
Epoch 4, Loss: 0.6106
Epoch 5, Loss: 0.6094
Epoch 6, Loss: 0.6085
Epoch 7, Loss: 0.6080
Epoch 8, Loss: 0.6076
Epoch 9, Loss: 0.6064
Epoch 10, Loss: 0.6077
Test set: Average loss: 0.0110, Accuracy: 353/515 (68.54%)
accuracy= 0.6854368932038835
accuracies= [0.6854368932038835, 0.6854368932038835, 0.6854368932038835, 0.6854368932038835, 0.6854368932038835, 0.6854368932038835]
Running Loop: 7/10
Epoch 1, Loss: 0.6218
Epoch 2, Loss: 0.6137
Epoch 3, Loss: 0.6119
Epoch 4, Loss: 0.6106
Epoch 5, Loss: 0.6109
Epoch 6, Loss: 0.6081
Epoch 7, Loss: 0.6077
Epoch 8, Loss: 0.6091
Epoch 9, Loss: 0.6076
Epoch 10, Loss: 0.6076
Test set: Average loss: 0.0110, Accuracy: 353/515 (68.54%)
accuracy= 0.6854368932038835
accuracies= [0.6854368932038835, 0.6854368932038835, 0.6854368932038835, 0.6854368932038835, 0.6854368932038835, 0.6854368932038835, 0.6854368932038835]
Running Loop: 8/10
Epoch 1, Loss: 0.6209
Epoch 2, Loss: 0.6112
Epoch 3, Loss: 0.6105
Epoch 4, Loss: 0.6106
Epoch 5, Loss: 0.6093
Epoch 6, Loss: 0.6100
Epoch 7, Loss: 0.6088
Epoch 8, Loss: 0.6082
Epoch 9, Loss: 0.6088
Epoch 10, Loss: 0.6109
Test set: Average loss: 0.0110, Accuracy: 353/515 (68.54%)
accuracy= 0.6854368932038835
accuracies= [0.6854368932038835, 0.6854368932038835, 0.6854368932038835, 0.6854368932038835, 0.6854368932038835, 0.6854368932038835, 0.6854368932038835, 0.6854368932038835]
Running Loop: 9/10
Epoch 1, Loss: 0.6168
Epoch 2, Loss: 0.6115
Epoch 3, Loss: 0.6100
Epoch 4, Loss: 0.6091
Epoch 5, Loss: 0.6094
Epoch 6, Loss: 0.6090
Epoch 7, Loss: 0.6087
Epoch 8, Loss: 0.6087
Epoch 9, Loss: 0.6082
Epoch 10, Loss: 0.6089
Test set: Average loss: 0.0110, Accuracy: 353/515 (68.54%)
accuracy= 0.6854368932038835
accuracies= [0.6854368932038835, 0.6854368932038835, 0.6854368932038835, 0.6854368932038835, 0.6854368932038835, 0.6854368932038835, 0.6854368932038835, 0.6854368932038835, 0.6854368932038835]
Running Loop: 10/10
Epoch 1, Loss: 0.6210
Epoch 2, Loss: 0.6119
Epoch 3, Loss: 0.6141
Epoch 4, Loss: 0.6121
Epoch 5, Loss: 0.6080
Epoch 6, Loss: 0.6081
Epoch 7, Loss: 0.6086
Epoch 8, Loss: 0.6080
Epoch 9, Loss: 0.6083
Epoch 10, Loss: 0.6082
Test set: Average loss: 0.0110, Accuracy: 353/515 (68.54%)
accuracy= 0.6854368932038835
accuracies= [0.6854368932038835, 0.6854368932038835, 0.6854368932038835, 0.6854368932038835, 0.6854368932038835, 0.6854368932038835, 0.6854368932038835, 0.6854368932038835, 0.6854368932038835, 0.6854368932038835]

Running CoLA Training with Optimizer = AdamW
params= {'lr': 0.001, 'betas': (0.9, 0.999), 'weight_decay': 0.01}
Running Loop: 1/10
Epoch 1, Loss: 0.6107
Epoch 2, Loss: 0.6083
Epoch 3, Loss: 0.6087
Epoch 4, Loss: 0.6110
Epoch 5, Loss: 0.6075
Epoch 6, Loss: 0.5999
Epoch 7, Loss: 0.5918
Epoch 8, Loss: 0.5846
Epoch 9, Loss: 0.5688
Epoch 10, Loss: 0.5547
Test set: Average loss: 0.0113, Accuracy: 341/515 (66.21%)
accuracy= 0.6621359223300971
accuracies= [0.6621359223300971]
Running Loop: 2/10
Epoch 1, Loss: 0.6112
Epoch 2, Loss: 0.6074
Epoch 3, Loss: 0.6050
Epoch 4, Loss: 0.5984
Epoch 5, Loss: 0.5933
Epoch 6, Loss: 0.5826
Epoch 7, Loss: 0.5691
Epoch 8, Loss: 0.5522
Epoch 9, Loss: 0.5350
Epoch 10, Loss: 0.5159
Test set: Average loss: 0.0125, Accuracy: 313/515 (60.78%)
accuracy= 0.6077669902912621
accuracies= [0.6621359223300971, 0.6077669902912621]
Running Loop: 3/10
Epoch 1, Loss: 0.6105
Epoch 2, Loss: 0.6080
Epoch 3, Loss: 0.6065
Epoch 4, Loss: 0.6033
Epoch 5, Loss: 0.5964
Epoch 6, Loss: 0.5906
Epoch 7, Loss: 0.5841
Epoch 8, Loss: 0.5702
Epoch 9, Loss: 0.5531
Epoch 10, Loss: 0.5406
Test set: Average loss: 0.0121, Accuracy: 329/515 (63.88%)
accuracy= 0.6388349514563106
accuracies= [0.6621359223300971, 0.6077669902912621, 0.6388349514563106]
Running Loop: 4/10
Epoch 1, Loss: 0.6109
Epoch 2, Loss: 0.6082
Epoch 3, Loss: 0.6081
Epoch 4, Loss: 0.6058
Epoch 5, Loss: 0.6027
Epoch 6, Loss: 0.5944
Epoch 7, Loss: 0.5865
Epoch 8, Loss: 0.5779
Epoch 9, Loss: 0.5593
Epoch 10, Loss: 0.5498
Test set: Average loss: 0.0116, Accuracy: 324/515 (62.91%)
accuracy= 0.629126213592233
accuracies= [0.6621359223300971, 0.6077669902912621, 0.6388349514563106, 0.629126213592233]
Running Loop: 5/10
Epoch 1, Loss: 0.6102
Epoch 2, Loss: 0.6076
Epoch 3, Loss: 0.6084
Epoch 4, Loss: 0.6025
Epoch 5, Loss: 0.5973
Epoch 6, Loss: 0.5840
Epoch 7, Loss: 0.5665
Epoch 8, Loss: 0.5504
Epoch 9, Loss: 0.5367
Epoch 10, Loss: 0.5190
Test set: Average loss: 0.0115, Accuracy: 331/515 (64.27%)
accuracy= 0.6427184466019418
accuracies= [0.6621359223300971, 0.6077669902912621, 0.6388349514563106, 0.629126213592233, 0.6427184466019418]
Running Loop: 6/10
Epoch 1, Loss: 0.6103
Epoch 2, Loss: 0.6064
Epoch 3, Loss: 0.6009
Epoch 4, Loss: 0.5921
Epoch 5, Loss: 0.5816
Epoch 6, Loss: 0.5676
Epoch 7, Loss: 0.5536
Epoch 8, Loss: 0.5396
Epoch 9, Loss: 0.5264
Epoch 10, Loss: 0.5139
Test set: Average loss: 0.0114, Accuracy: 313/515 (60.78%)
accuracy= 0.6077669902912621
accuracies= [0.6621359223300971, 0.6077669902912621, 0.6388349514563106, 0.629126213592233, 0.6427184466019418, 0.6077669902912621]
Running Loop: 7/10
Epoch 1, Loss: 0.6102
Epoch 2, Loss: 0.6067
Epoch 3, Loss: 0.6068
Epoch 4, Loss: 0.6014
Epoch 5, Loss: 0.5995
Epoch 6, Loss: 0.5883
Epoch 7, Loss: 0.5820
Epoch 8, Loss: 0.5718
Epoch 9, Loss: 0.5537
Epoch 10, Loss: 0.5403
Test set: Average loss: 0.0107, Accuracy: 341/515 (66.21%)
accuracy= 0.6621359223300971
accuracies= [0.6621359223300971, 0.6077669902912621, 0.6388349514563106, 0.629126213592233, 0.6427184466019418, 0.6077669902912621, 0.6621359223300971]
Running Loop: 8/10
Epoch 1, Loss: 0.6103
Epoch 2, Loss: 0.6085
Epoch 3, Loss: 0.6079
Epoch 4, Loss: 0.6062
Epoch 5, Loss: 0.6046
Epoch 6, Loss: 0.6009
Epoch 7, Loss: 0.5914
Epoch 8, Loss: 0.5839
Epoch 9, Loss: 0.5713
Epoch 10, Loss: 0.5608
Test set: Average loss: 0.0118, Accuracy: 317/515 (61.55%)
accuracy= 0.6155339805825243
accuracies= [0.6621359223300971, 0.6077669902912621, 0.6388349514563106, 0.629126213592233, 0.6427184466019418, 0.6077669902912621, 0.6621359223300971, 0.6155339805825243]
Running Loop: 9/10
Epoch 1, Loss: 0.6103
Epoch 2, Loss: 0.6068
Epoch 3, Loss: 0.6062
Epoch 4, Loss: 0.6008
Epoch 5, Loss: 0.5899
Epoch 6, Loss: 0.5791
Epoch 7, Loss: 0.5702
Epoch 8, Loss: 0.5578
Epoch 9, Loss: 0.5427
Epoch 10, Loss: 0.5312
Test set: Average loss: 0.0111, Accuracy: 327/515 (63.50%)
accuracy= 0.6349514563106796
accuracies= [0.6621359223300971, 0.6077669902912621, 0.6388349514563106, 0.629126213592233, 0.6427184466019418, 0.6077669902912621, 0.6621359223300971, 0.6155339805825243, 0.6349514563106796]
Running Loop: 10/10
Epoch 1, Loss: 0.6109
Epoch 2, Loss: 0.6092
Epoch 3, Loss: 0.6064
Epoch 4, Loss: 0.6019
Epoch 5, Loss: 0.5990
Epoch 6, Loss: 0.5880
Epoch 7, Loss: 0.5719
Epoch 8, Loss: 0.5574
Epoch 9, Loss: 0.5431
Epoch 10, Loss: 0.5260
Test set: Average loss: 0.0108, Accuracy: 348/515 (67.57%)
accuracy= 0.6757281553398058
accuracies= [0.6621359223300971, 0.6077669902912621, 0.6388349514563106, 0.629126213592233, 0.6427184466019418, 0.6077669902912621, 0.6621359223300971, 0.6155339805825243, 0.6349514563106796, 0.6757281553398058]

Running CoLA Training with Optimizer = NAdam
params= {'lr': 0.001, 'betas': (0.9, 0.999)}
Running Loop: 1/10
Epoch 1, Loss: 0.6104
Epoch 2, Loss: 0.6109
Epoch 3, Loss: 0.6090
Epoch 4, Loss: 0.6072
Epoch 5, Loss: 0.6041
Epoch 6, Loss: 0.6002
Epoch 7, Loss: 0.5935
Epoch 8, Loss: 0.5821
Epoch 9, Loss: 0.5683
Epoch 10, Loss: 0.5600
Test set: Average loss: 0.0111, Accuracy: 344/515 (66.80%)
accuracy= 0.6679611650485436
accuracies= [0.6679611650485436]
Running Loop: 2/10
Epoch 1, Loss: 0.6121
Epoch 2, Loss: 0.6082
Epoch 3, Loss: 0.6051
Epoch 4, Loss: 0.6053
Epoch 5, Loss: 0.6018
Epoch 6, Loss: 0.5894
Epoch 7, Loss: 0.5799
Epoch 8, Loss: 0.5682
Epoch 9, Loss: 0.5548
Epoch 10, Loss: 0.5471
Test set: Average loss: 0.0124, Accuracy: 304/515 (59.03%)
accuracy= 0.5902912621359223
accuracies= [0.6679611650485436, 0.5902912621359223]
Running Loop: 3/10
Epoch 1, Loss: 0.6163
Epoch 2, Loss: 0.6084
Epoch 3, Loss: 0.6062
Epoch 4, Loss: 0.6051
Epoch 5, Loss: 0.5996
Epoch 6, Loss: 0.5915
Epoch 7, Loss: 0.5810
Epoch 8, Loss: 0.5695
Epoch 9, Loss: 0.5599
Epoch 10, Loss: 0.5460
Test set: Average loss: 0.0107, Accuracy: 345/515 (66.99%)
accuracy= 0.6699029126213593
accuracies= [0.6679611650485436, 0.5902912621359223, 0.6699029126213593]
Running Loop: 4/10
Epoch 1, Loss: 0.6134
Epoch 2, Loss: 0.6075
Epoch 3, Loss: 0.6047
Epoch 4, Loss: 0.6010
Epoch 5, Loss: 0.5945
Epoch 6, Loss: 0.5883
Epoch 7, Loss: 0.5753
Epoch 8, Loss: 0.5678
Epoch 9, Loss: 0.5568
Epoch 10, Loss: 0.5476
Test set: Average loss: 0.0109, Accuracy: 337/515 (65.44%)
accuracy= 0.654368932038835
accuracies= [0.6679611650485436, 0.5902912621359223, 0.6699029126213593, 0.654368932038835]
Running Loop: 5/10
Epoch 1, Loss: 0.6105
Epoch 2, Loss: 0.6111
Epoch 3, Loss: 0.6085
Epoch 4, Loss: 0.6071
Epoch 5, Loss: 0.6078
Epoch 6, Loss: 0.6032
Epoch 7, Loss: 0.5972
Epoch 8, Loss: 0.5922
Epoch 9, Loss: 0.5835
Epoch 10, Loss: 0.5714
Test set: Average loss: 0.0107, Accuracy: 336/515 (65.24%)
accuracy= 0.6524271844660194
accuracies= [0.6679611650485436, 0.5902912621359223, 0.6699029126213593, 0.654368932038835, 0.6524271844660194]
Running Loop: 6/10
Epoch 1, Loss: 0.6099
Epoch 2, Loss: 0.6063
Epoch 3, Loss: 0.6113
Epoch 4, Loss: 0.6060
Epoch 5, Loss: 0.6011
Epoch 6, Loss: 0.5928
Epoch 7, Loss: 0.5866
Epoch 8, Loss: 0.5737
Epoch 9, Loss: 0.5641
Epoch 10, Loss: 0.5521
Test set: Average loss: 0.0106, Accuracy: 348/515 (67.57%)
accuracy= 0.6757281553398058
accuracies= [0.6679611650485436, 0.5902912621359223, 0.6699029126213593, 0.654368932038835, 0.6524271844660194, 0.6757281553398058]
Running Loop: 7/10
Epoch 1, Loss: 0.6123
Epoch 2, Loss: 0.6068
Epoch 3, Loss: 0.6047
Epoch 4, Loss: 0.5976
Epoch 5, Loss: 0.5923
Epoch 6, Loss: 0.5835
Epoch 7, Loss: 0.5753
Epoch 8, Loss: 0.5604
Epoch 9, Loss: 0.5557
Epoch 10, Loss: 0.5379
Test set: Average loss: 0.0115, Accuracy: 330/515 (64.08%)
accuracy= 0.6407766990291263
accuracies= [0.6679611650485436, 0.5902912621359223, 0.6699029126213593, 0.654368932038835, 0.6524271844660194, 0.6757281553398058, 0.6407766990291263]
Running Loop: 8/10
Epoch 1, Loss: 0.6104
Epoch 2, Loss: 0.6093
Epoch 3, Loss: 0.6077
Epoch 4, Loss: 0.6077
Epoch 5, Loss: 0.6082
Epoch 6, Loss: 0.6033
Epoch 7, Loss: 0.6038
Epoch 8, Loss: 0.5989
Epoch 9, Loss: 0.5941
Epoch 10, Loss: 0.5794
Test set: Average loss: 0.0113, Accuracy: 327/515 (63.50%)
accuracy= 0.6349514563106796
accuracies= [0.6679611650485436, 0.5902912621359223, 0.6699029126213593, 0.654368932038835, 0.6524271844660194, 0.6757281553398058, 0.6407766990291263, 0.6349514563106796]
Running Loop: 9/10
Epoch 1, Loss: 0.6103
Epoch 2, Loss: 0.6088
Epoch 3, Loss: 0.6090
Epoch 4, Loss: 0.6062
Epoch 5, Loss: 0.6052
Epoch 6, Loss: 0.6044
Epoch 7, Loss: 0.5964
Epoch 8, Loss: 0.5904
Epoch 9, Loss: 0.5811
Epoch 10, Loss: 0.5689
Test set: Average loss: 0.0110, Accuracy: 342/515 (66.41%)
accuracy= 0.6640776699029126
accuracies= [0.6679611650485436, 0.5902912621359223, 0.6699029126213593, 0.654368932038835, 0.6524271844660194, 0.6757281553398058, 0.6407766990291263, 0.6349514563106796, 0.6640776699029126]
Running Loop: 10/10
Epoch 1, Loss: 0.6107
Epoch 2, Loss: 0.6112
Epoch 3, Loss: 0.6095
Epoch 4, Loss: 0.6071
Epoch 5, Loss: 0.6063
Epoch 6, Loss: 0.6073
Epoch 7, Loss: 0.6032
Epoch 8, Loss: 0.5962
Epoch 9, Loss: 0.5886
Epoch 10, Loss: 0.5774
Test set: Average loss: 0.0109, Accuracy: 327/515 (63.50%)
accuracy= 0.6349514563106796
accuracies= [0.6679611650485436, 0.5902912621359223, 0.6699029126213593, 0.654368932038835, 0.6524271844660194, 0.6757281553398058, 0.6407766990291263, 0.6349514563106796, 0.6640776699029126, 0.6349514563106796]

Running CoLA Training with Optimizer = NAdamW
params= {'lr': 0.001, 'betas': (0.9, 0.999), 'weight_decay': 0.01}
Running Loop: 1/10
Epoch 1, Loss: 0.6104
Epoch 2, Loss: 0.6114
Epoch 3, Loss: 0.6087
Epoch 4, Loss: 0.6066
Epoch 5, Loss: 0.6060
Epoch 6, Loss: 0.6053
Epoch 7, Loss: 0.6009
Epoch 8, Loss: 0.5936
Epoch 9, Loss: 0.5853
Epoch 10, Loss: 0.5766
Test set: Average loss: 0.0108, Accuracy: 333/515 (64.66%)
accuracy= 0.6466019417475728
accuracies= [0.6466019417475728]
Running Loop: 2/10
Epoch 1, Loss: 0.6125
Epoch 2, Loss: 0.6084
Epoch 3, Loss: 0.6068
Epoch 4, Loss: 0.6039
Epoch 5, Loss: 0.6016
Epoch 6, Loss: 0.5962
Epoch 7, Loss: 0.5891
Epoch 8, Loss: 0.5782
Epoch 9, Loss: 0.5690
Epoch 10, Loss: 0.5557
Test set: Average loss: 0.0111, Accuracy: 350/515 (67.96%)
accuracy= 0.6796116504854369
accuracies= [0.6466019417475728, 0.6796116504854369]
Running Loop: 3/10
Epoch 1, Loss: 0.6149
Epoch 2, Loss: 0.6085
Epoch 3, Loss: 0.6079
Epoch 4, Loss: 0.6073
Epoch 5, Loss: 0.6064
Epoch 6, Loss: 0.6030
Epoch 7, Loss: 0.5988
Epoch 8, Loss: 0.5871
Epoch 9, Loss: 0.5745
Epoch 10, Loss: 0.5582
Test set: Average loss: 0.0117, Accuracy: 346/515 (67.18%)
accuracy= 0.6718446601941748
accuracies= [0.6466019417475728, 0.6796116504854369, 0.6718446601941748]
Running Loop: 4/10
Epoch 1, Loss: 0.6126
Epoch 2, Loss: 0.6086
Epoch 3, Loss: 0.6089
Epoch 4, Loss: 0.6055
Epoch 5, Loss: 0.6055
Epoch 6, Loss: 0.6020
Epoch 7, Loss: 0.6001
Epoch 8, Loss: 0.5947
Epoch 9, Loss: 0.5846
Epoch 10, Loss: 0.5747
Test set: Average loss: 0.0108, Accuracy: 332/515 (64.47%)
accuracy= 0.6446601941747573
accuracies= [0.6466019417475728, 0.6796116504854369, 0.6718446601941748, 0.6446601941747573]
Running Loop: 5/10
Epoch 1, Loss: 0.6108
Epoch 2, Loss: 0.6082
Epoch 3, Loss: 0.6086
Epoch 4, Loss: 0.6069
Epoch 5, Loss: 0.6063
Epoch 6, Loss: 0.6020
Epoch 7, Loss: 0.5967
Epoch 8, Loss: 0.5847
Epoch 9, Loss: 0.5833
Epoch 10, Loss: 0.5702
Test set: Average loss: 0.0112, Accuracy: 334/515 (64.85%)
accuracy= 0.6485436893203883
accuracies= [0.6466019417475728, 0.6796116504854369, 0.6718446601941748, 0.6446601941747573, 0.6485436893203883]
Running Loop: 6/10
Epoch 1, Loss: 0.6099
Epoch 2, Loss: 0.6078
Epoch 3, Loss: 0.6069
Epoch 4, Loss: 0.6028
Epoch 5, Loss: 0.6002
Epoch 6, Loss: 0.5955
Epoch 7, Loss: 0.5898
Epoch 8, Loss: 0.5753
Epoch 9, Loss: 0.5642
Epoch 10, Loss: 0.5545
Test set: Average loss: 0.0111, Accuracy: 337/515 (65.44%)
accuracy= 0.654368932038835
accuracies= [0.6466019417475728, 0.6796116504854369, 0.6718446601941748, 0.6446601941747573, 0.6485436893203883, 0.654368932038835]
Running Loop: 7/10
Epoch 1, Loss: 0.6116
Epoch 2, Loss: 0.6075
Epoch 3, Loss: 0.6081
Epoch 4, Loss: 0.6089
Epoch 5, Loss: 0.6053
Epoch 6, Loss: 0.6013
Epoch 7, Loss: 0.5939
Epoch 8, Loss: 0.5891
Epoch 9, Loss: 0.5733
Epoch 10, Loss: 0.5613
Test set: Average loss: 0.0109, Accuracy: 329/515 (63.88%)
accuracy= 0.6388349514563106
accuracies= [0.6466019417475728, 0.6796116504854369, 0.6718446601941748, 0.6446601941747573, 0.6485436893203883, 0.654368932038835, 0.6388349514563106]
Running Loop: 8/10
Epoch 1, Loss: 0.6104
Epoch 2, Loss: 0.6140
Epoch 3, Loss: 0.6073
Epoch 4, Loss: 0.6080
Epoch 5, Loss: 0.6058
Epoch 6, Loss: 0.6064
Epoch 7, Loss: 0.6036
Epoch 8, Loss: 0.5991
Epoch 9, Loss: 0.5945
Epoch 10, Loss: 0.5877
Test set: Average loss: 0.0113, Accuracy: 338/515 (65.63%)
accuracy= 0.6563106796116505
accuracies= [0.6466019417475728, 0.6796116504854369, 0.6718446601941748, 0.6446601941747573, 0.6485436893203883, 0.654368932038835, 0.6388349514563106, 0.6563106796116505]
Running Loop: 9/10
Epoch 1, Loss: 0.6103
Epoch 2, Loss: 0.6090
Epoch 3, Loss: 0.6090
Epoch 4, Loss: 0.6082
Epoch 5, Loss: 0.6063
Epoch 6, Loss: 0.6014
Epoch 7, Loss: 0.5943
Epoch 8, Loss: 0.5841
Epoch 9, Loss: 0.5751
Epoch 10, Loss: 0.5662
Test set: Average loss: 0.0115, Accuracy: 333/515 (64.66%)
accuracy= 0.6466019417475728
accuracies= [0.6466019417475728, 0.6796116504854369, 0.6718446601941748, 0.6446601941747573, 0.6485436893203883, 0.654368932038835, 0.6388349514563106, 0.6563106796116505, 0.6466019417475728]
Running Loop: 10/10
Epoch 1, Loss: 0.6105
Epoch 2, Loss: 0.6105
Epoch 3, Loss: 0.6082
Epoch 4, Loss: 0.6080
Epoch 5, Loss: 0.6023
Epoch 6, Loss: 0.6000
Epoch 7, Loss: 0.5914
Epoch 8, Loss: 0.5788
Epoch 9, Loss: 0.5692
Epoch 10, Loss: 0.5575
Test set: Average loss: 0.0110, Accuracy: 358/515 (69.51%)
accuracy= 0.6951456310679611
accuracies= [0.6466019417475728, 0.6796116504854369, 0.6718446601941748, 0.6446601941747573, 0.6485436893203883, 0.654368932038835, 0.6388349514563106, 0.6563106796116505, 0.6466019417475728, 0.6951456310679611]

Running CoLA Training with Optimizer = AMSGrad
params= {'lr': 0.001, 'betas': (0.9, 0.999)}
Running Loop: 1/10
Epoch 1, Loss: 0.6107
Epoch 2, Loss: 0.6094
Epoch 3, Loss: 0.6061
Epoch 4, Loss: 0.6028
Epoch 5, Loss: 0.5935
Epoch 6, Loss: 0.5817
Epoch 7, Loss: 0.5766
Epoch 8, Loss: 0.5632
Epoch 9, Loss: 0.5448
Epoch 10, Loss: 0.5313
Test set: Average loss: 0.0114, Accuracy: 338/515 (65.63%)
accuracy= 0.6563106796116505
accuracies= [0.6563106796116505]
Running Loop: 2/10
Epoch 1, Loss: 0.6112
Epoch 2, Loss: 0.6076
Epoch 3, Loss: 0.6054
Epoch 4, Loss: 0.5987
Epoch 5, Loss: 0.5914
Epoch 6, Loss: 0.5807
Epoch 7, Loss: 0.5666
Epoch 8, Loss: 0.5494
Epoch 9, Loss: 0.5257
Epoch 10, Loss: 0.5101
Test set: Average loss: 0.0119, Accuracy: 316/515 (61.36%)
accuracy= 0.6135922330097088
accuracies= [0.6563106796116505, 0.6135922330097088]
Running Loop: 3/10
Epoch 1, Loss: 0.6105
Epoch 2, Loss: 0.6080
Epoch 3, Loss: 0.6077
Epoch 4, Loss: 0.6056
Epoch 5, Loss: 0.6018
Epoch 6, Loss: 0.5946
Epoch 7, Loss: 0.5888
Epoch 8, Loss: 0.5693
Epoch 9, Loss: 0.5563
Epoch 10, Loss: 0.5406
Test set: Average loss: 0.0111, Accuracy: 329/515 (63.88%)
accuracy= 0.6388349514563106
accuracies= [0.6563106796116505, 0.6135922330097088, 0.6388349514563106]
Running Loop: 4/10
Epoch 1, Loss: 0.6109
Epoch 2, Loss: 0.6082
Epoch 3, Loss: 0.6069
Epoch 4, Loss: 0.6026
Epoch 5, Loss: 0.6027
Epoch 6, Loss: 0.5979
Epoch 7, Loss: 0.5874
Epoch 8, Loss: 0.5834
Epoch 9, Loss: 0.5684
Epoch 10, Loss: 0.5576
Test set: Average loss: 0.0106, Accuracy: 341/515 (66.21%)
accuracy= 0.6621359223300971
accuracies= [0.6563106796116505, 0.6135922330097088, 0.6388349514563106, 0.6621359223300971]
Running Loop: 5/10
Epoch 1, Loss: 0.6102
Epoch 2, Loss: 0.6074
Epoch 3, Loss: 0.6058
Epoch 4, Loss: 0.5989
Epoch 5, Loss: 0.5931
Epoch 6, Loss: 0.5810
Epoch 7, Loss: 0.5672
Epoch 8, Loss: 0.5549
Epoch 9, Loss: 0.5339
Epoch 10, Loss: 0.5181
Test set: Average loss: 0.0108, Accuracy: 334/515 (64.85%)
accuracy= 0.6485436893203883
accuracies= [0.6563106796116505, 0.6135922330097088, 0.6388349514563106, 0.6621359223300971, 0.6485436893203883]
Running Loop: 6/10
Epoch 1, Loss: 0.6103
Epoch 2, Loss: 0.6100
Epoch 3, Loss: 0.6048
Epoch 4, Loss: 0.5994
Epoch 5, Loss: 0.5880
Epoch 6, Loss: 0.5814
Epoch 7, Loss: 0.5644
Epoch 8, Loss: 0.5507
Epoch 9, Loss: 0.5310
Epoch 10, Loss: 0.5159
Test set: Average loss: 0.0116, Accuracy: 323/515 (62.72%)
accuracy= 0.6271844660194175
accuracies= [0.6563106796116505, 0.6135922330097088, 0.6388349514563106, 0.6621359223300971, 0.6485436893203883, 0.6271844660194175]
Running Loop: 7/10
Epoch 1, Loss: 0.6102
Epoch 2, Loss: 0.6067
Epoch 3, Loss: 0.6066
Epoch 4, Loss: 0.6033
Epoch 5, Loss: 0.5924
Epoch 6, Loss: 0.5788
Epoch 7, Loss: 0.5662
Epoch 8, Loss: 0.5470
Epoch 9, Loss: 0.5286
Epoch 10, Loss: 0.5146
Test set: Average loss: 0.0110, Accuracy: 323/515 (62.72%)
accuracy= 0.6271844660194175
accuracies= [0.6563106796116505, 0.6135922330097088, 0.6388349514563106, 0.6621359223300971, 0.6485436893203883, 0.6271844660194175, 0.6271844660194175]
Running Loop: 8/10
Epoch 1, Loss: 0.6103
Epoch 2, Loss: 0.6085
Epoch 3, Loss: 0.6072
Epoch 4, Loss: 0.6070
Epoch 5, Loss: 0.6064
Epoch 6, Loss: 0.6053
Epoch 7, Loss: 0.6041
Epoch 8, Loss: 0.5983
Epoch 9, Loss: 0.5875
Epoch 10, Loss: 0.5791
Test set: Average loss: 0.0118, Accuracy: 330/515 (64.08%)
accuracy= 0.6407766990291263
accuracies= [0.6563106796116505, 0.6135922330097088, 0.6388349514563106, 0.6621359223300971, 0.6485436893203883, 0.6271844660194175, 0.6271844660194175, 0.6407766990291263]
Running Loop: 9/10
Epoch 1, Loss: 0.6103
Epoch 2, Loss: 0.6071
Epoch 3, Loss: 0.6065
Epoch 4, Loss: 0.5945
Epoch 5, Loss: 0.5872
Epoch 6, Loss: 0.5737
Epoch 7, Loss: 0.5548
Epoch 8, Loss: 0.5439
Epoch 9, Loss: 0.5326
Epoch 10, Loss: 0.5143
Test set: Average loss: 0.0108, Accuracy: 332/515 (64.47%)
accuracy= 0.6446601941747573
accuracies= [0.6563106796116505, 0.6135922330097088, 0.6388349514563106, 0.6621359223300971, 0.6485436893203883, 0.6271844660194175, 0.6271844660194175, 0.6407766990291263, 0.6446601941747573]
Running Loop: 10/10
Epoch 1, Loss: 0.6109
Epoch 2, Loss: 0.6075
Epoch 3, Loss: 0.6059
Epoch 4, Loss: 0.6040
Epoch 5, Loss: 0.5999
Epoch 6, Loss: 0.5853
Epoch 7, Loss: 0.5755
Epoch 8, Loss: 0.5607
Epoch 9, Loss: 0.5428
Epoch 10, Loss: 0.5295
Test set: Average loss: 0.0113, Accuracy: 333/515 (64.66%)
accuracy= 0.6466019417475728
accuracies= [0.6563106796116505, 0.6135922330097088, 0.6388349514563106, 0.6621359223300971, 0.6485436893203883, 0.6271844660194175, 0.6271844660194175, 0.6407766990291263, 0.6446601941747573, 0.6466019417475728]

Running CoLA Training with Optimizer = Shampoo
params= {'lr': 0.001, 'momentum': 0.1}
Running Loop: 1/10
